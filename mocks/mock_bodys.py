markov_decision_process_body = '<link rel="stylesheet" href="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3563198/dp_colors_uit_blue.css"><div class="mw-page-container">\n<div class="mw-page-container-inner">\n<div class="mw-content-container">\n<div id="bodyContent" class="vector-body ve-init-mw-desktopArticleTarget-targetContainer" aria-labelledby="firstHeading" data-mw-ve-target-container="">\n<div id="mw-content-text" class="mw-body-content">\n<div class="mw-content-ltr mw-parser-output" dir="ltr" lang="en">\n<p><strong>Markov decision process</strong><span>&nbsp;</span>(<strong>MDP</strong>), also called a<span>&nbsp;</span><a title="Stochastic dynamic programming" href="https://en.wikipedia.org/wiki/Stochastic_dynamic_programming">stochastic dynamic program</a><span>&nbsp;</span>or stochastic control problem, is a model for<span>&nbsp;</span><a title="Sequential decision making" href="https://en.wikipedia.org/wiki/Sequential_decision_making">sequential decision making</a><span>&nbsp;</span>when<span>&nbsp;</span><a title="Outcome (probability)" href="https://en.wikipedia.org/wiki/Outcome_(probability)">outcomes</a><span>&nbsp;</span>are uncertain.<sup id="cite_ref-1" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup></p>\n<p>Originating from<span>&nbsp;</span><a title="Operations research" href="https://en.wikipedia.org/wiki/Operations_research">operations research</a><span>&nbsp;</span>in the 1950s,<sup id="cite_ref-2" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-2"><span class="cite-bracket">[</span>2<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-3" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>MDPs have since gained recognition in a variety of fields, including<span>&nbsp;</span><a title="Ecology" href="https://en.wikipedia.org/wiki/Ecology">ecology</a>,<span>&nbsp;</span><a title="Economics" href="https://en.wikipedia.org/wiki/Economics">economics</a>,<span>&nbsp;</span><a title="Health care" href="https://en.wikipedia.org/wiki/Health_care">healthcare</a>,<span>&nbsp;</span><a title="Telecommunications" href="https://en.wikipedia.org/wiki/Telecommunications">telecommunications</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Reinforcement learning" href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>.<sup id="cite_ref-:0_4-0" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-:0-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Reinforcement learning utilizes the MDP framework to model the interaction between a learning agent and its environment. In this framework, the interaction is characterized by states, actions, and rewards. The MDP framework is designed to provide a simplified representation of key elements of<span>&nbsp;</span><a title="Artificial intelligence" href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a><span>&nbsp;</span>challenges. These elements encompass the understanding of<span>&nbsp;</span><a title="Causality" href="https://en.wikipedia.org/wiki/Causality">cause and effect</a>, the management of uncertainty and nondeterminism, and the pursuit of explicit goals.<sup id="cite_ref-:0_4-1" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-:0-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup></p>\n<p>The name comes from its connection to<span>&nbsp;</span><a title="Markov chain" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, a concept developed by the Russian mathematician<span>&nbsp;</span><a title="Andrey Markov" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>. The "Markov" in "Markov decision process" refers to the underlying structure of<span>&nbsp;</span><a title="Transition system" href="https://en.wikipedia.org/wiki/Transition_system">state transitions</a><span>&nbsp;</span>that still follow the<span>&nbsp;</span><a title="Markov property" href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>. The process is called a "decision process" because it involves making decisions that influence these state transitions, extending the concept of a Markov chain into the realm of decision-making under uncertainty.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Definition">Definition</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Definition" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=1"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<figure><a class="mw-file-description" href="https://en.wikipedia.org/wiki/File:Markov_Decision_Process.svg"><img class="mw-file-element" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/500px-Markov_Decision_Process.svg.png" width="400" height="320" data-file-width="800" data-file-height="640" loading="lazy"></a>\n<figcaption>Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows)</figcaption>\n</figure>\n<p>A Markov decision process is a 4-<a title="Tuple" href="https://en.wikipedia.org/wiki/Tuple">tuple</a><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (S,A,P_{a},R_{a})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo>,</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4087b8e195d2e67af8b20e9ed556b697653500c" alt="{\\displaystyle (S,A,P_{a},R_{a})}" aria-hidden="true" loading="lazy"></span>, where:</p>\n<ul>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>S</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" alt="{\\displaystyle S}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a<span>&nbsp;</span><a title="Set (mathematics)" href="https://en.wikipedia.org/wiki/Set_(mathematics)">set</a><span>&nbsp;</span>of states called the<span>&nbsp;</span><i><dfn>state space</dfn></i>. The state space may be discrete or continuous, like the<span>&nbsp;</span><a class="mw-redirect" title="Set of real numbers" href="https://en.wikipedia.org/wiki/Set_of_real_numbers">set of real numbers</a>.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle A}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>A</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" alt="{\\displaystyle A}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a set of actions called the<span>&nbsp;</span><i><dfn>action space</dfn></i><span>&nbsp;</span>(alternatively,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle A_{s}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cc9b664ef7e1dca131e7f345b4321bd3a07a7d8" alt="{\\displaystyle A_{s}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the set of actions available from state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>). As for state, this set may be discrete or continuous.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c4b70b80a8fc6447e0804ce186459e40faa38db2" alt="{\\displaystyle P_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is, on an intuitive level, the probability that action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>at time<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle t}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>t</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" alt="{\\displaystyle t}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>will lead to state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>at time<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle t+1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>t</mi><mo>+</mo><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ab2785d8415d6902b0c93efe1419c4bc3ce4643d" alt="{\\displaystyle t+1}" aria-hidden="true" loading="lazy"></span>. In general, this probability transition is defined to satisfy<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\Pr(s_{t+1}\\in S\'\\mid s_{t}=s,a_{t}=a)=\\int _{S\'}P_{a}(s,s\')ds\',}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∈</mo><msup><mi>S</mi><mo>′</mo></msup><mo>∣</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∫</mo><mrow class="MJX-TeXAtom-ORD"><msup><mi>S</mi><mo>′</mo></msup></mrow></msub><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mi>d</mi><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c12fd484942a3a89bbeb79295a0c69f96d5e0b27" alt="{\\displaystyle \\Pr(s_{t+1}\\in S\'\\mid s_{t}=s,a_{t}=a)=\\int _{S\'}P_{a}(s,s\')ds\',}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>for every<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S\'\\subseteq S}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>S</mi><mo>′</mo></msup><mo>⊆</mo><mi>S</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/73992fdbc364d2a7ab0bd0161d815c668b2cc1de" alt="{\\displaystyle S\'\\subseteq S}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>measurable. In case the state space is discrete, the integral is intended with respect to the counting measure, so that the latter simplifies as<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')=\\Pr(s_{t+1}=s\'\\mid s_{t}=s,a_{t}=a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>s</mi><mo>′</mo></msup><mo>∣</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f46bb2846b8b20a08220507fa105540f34850e7" alt="{\\displaystyle P_{a}(s,s\')=\\Pr(s_{t+1}=s\'\\mid s_{t}=s,a_{t}=a)}" aria-hidden="true" loading="lazy"></span>; In case<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S\\subseteq \\mathbb {R} ^{d}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>S</mi><mo>⊆</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/78b77f0eee0dc1be8ba51d6c0b2ffeb364a1194f" alt="{\\displaystyle S\\subseteq \\mathbb {R} ^{d}}" aria-hidden="true" loading="lazy"></span>, the integral is usually intended with respect to the<span>&nbsp;</span><a title="Lebesgue measure" href="https://en.wikipedia.org/wiki/Lebesgue_measure">Lebesgue measure</a>.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f842d4a4b1340e194d8014d63163e5e27f94215" alt="{\\displaystyle R_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the immediate reward (or expected immediate reward) received after transitioning from state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span>, due to action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span>.</li>\n</ul>\n<p>A policy function<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a (potentially probabilistic) mapping from state space (<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>S</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" alt="{\\displaystyle S}" aria-hidden="true" loading="lazy"></span>) to action space (<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle A}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>A</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" alt="{\\displaystyle A}" aria-hidden="true" loading="lazy"></span>).</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Optimization_objective">Optimization objective</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Optimization objective" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=2"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The goal in a Markov decision process is to find a good "policy" for the decision maker: a function<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>that specifies the action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>that the decision maker will choose when in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>. Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a<span>&nbsp;</span><a title="Markov chain" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a><span>&nbsp;</span>(since the action chosen in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is completely determined by<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span>).</p>\n<p>The objective is to choose a policy<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle E\\left[\\sum _{t=0}^{\\infty }{\\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\\right]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>E</mi><mrow><mo>[</mo><mrow><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><msup><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/712f6adf8034f0df53957e45ab0f35441bef017e" alt="{\\displaystyle E\\left[\\sum _{t=0}^{\\infty }{\\gamma ^{t}R_{a_{t}}(s_{t},s_{t+1})}\\right]}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>(where we choose<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a_{t}=\\pi (s_{t})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e26eb7f6c69e62ff95c5b35469e3a3ae2cdf7c80" alt="{\\displaystyle a_{t}=\\pi (s_{t})}" aria-hidden="true" loading="lazy"></span>, i.e. actions given by the policy). And the expectation is taken over<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bc4c52832aa017edec5edd8eb7fee859eea58722" alt="{\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\ \\gamma \\ }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mtext>&nbsp;</mtext><mi>γ</mi><mtext>&nbsp;</mtext></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c7234066f70f79e0ba5ebf9602e759977a55014b" alt="{\\displaystyle \\ \\gamma \\ }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the discount factor satisfying<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 0\\leq \\ \\gamma \\ \\leq \\ 1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn><mo>≤</mo><mtext>&nbsp;</mtext><mi>γ</mi><mtext>&nbsp;</mtext><mo>≤</mo><mtext>&nbsp;</mtext><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fd937e4dae3a7ef276f50447848309947e955b5" alt="{\\displaystyle 0\\leq \\ \\gamma \\ \\leq \\ 1}" aria-hidden="true" loading="lazy"></span>, which is usually close to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92d98b82a3778f043108d4e20960a9193df57cbf" alt="{\\displaystyle 1}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>(for example,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\gamma =1/(1+r)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>γ</mi><mo>=</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>r</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/436f172e889f48179cbaeb79b6bcf80d7bbba95f" alt="{\\displaystyle \\gamma =1/(1+r)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>for some discount rate<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle r}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>r</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538" alt="{\\displaystyle r}" aria-hidden="true" loading="lazy"></span>). A lower discount factor motivates the decision maker to favor taking actions early, rather than postpone them indefinitely.</p>\n<p>Another possible, but strictly related, objective that is commonly used is the<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle H-}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>H</mi><mo>−</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6db8d448fa19fa3c7949aca3998754028cfb6654" alt="{\\displaystyle H-}" aria-hidden="true" loading="lazy"></span>step return. This time, instead of using a discount factor<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\ \\gamma \\ }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mtext>&nbsp;</mtext><mi>γ</mi><mtext>&nbsp;</mtext></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c7234066f70f79e0ba5ebf9602e759977a55014b" alt="{\\displaystyle \\ \\gamma \\ }" aria-hidden="true" loading="lazy"></span>, the agent is interested only in the first<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle H}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>H</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b" alt="{\\displaystyle H}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>steps of the process, with each reward having the same weight.</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle E\\left[\\sum _{t=0}^{H-1}{R_{a_{t}}(s_{t},s_{t+1})}\\right]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>E</mi><mrow><mo>[</mo><mrow><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>H</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow class="MJX-TeXAtom-ORD"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo>]</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fed17ad5f37ef941a776016fb4ba8c680697f44" alt="{\\displaystyle E\\left[\\sum _{t=0}^{H-1}{R_{a_{t}}(s_{t},s_{t+1})}\\right]}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>(where we choose<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a_{t}=\\pi (s_{t})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e26eb7f6c69e62ff95c5b35469e3a3ae2cdf7c80" alt="{\\displaystyle a_{t}=\\pi (s_{t})}" aria-hidden="true" loading="lazy"></span>, i.e. actions given by the policy). And the expectation is taken over<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∼</mo><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><msub><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bc4c52832aa017edec5edd8eb7fee859eea58722" alt="{\\displaystyle s_{t+1}\\sim P_{a_{t}}(s_{t},s_{t+1})}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\ H\\ }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mtext>&nbsp;</mtext><mi>H</mi><mtext>&nbsp;</mtext></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b34556162a24c14754c402daf0f582a5c76dab03" alt="{\\displaystyle \\ H\\ }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the time horizon. Compared to the previous objective, the latter one is more used in<span>&nbsp;</span><a class="new" title="Learning Theory (page does not exist)" href="https://en.wikipedia.org/w/index.php?title=Learning_Theory&amp;action=edit&amp;redlink=1">Learning Theory</a>.</p>\n<p>A policy that maximizes the function above is called an<span>&nbsp;</span><i><dfn>optimal policy</dfn></i><span>&nbsp;</span>and is usually denoted<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi ^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f44ad69ec033a9a86437b2edaf620ea0b2c3f494" alt="{\\displaystyle \\pi ^{*}}" aria-hidden="true" loading="lazy"></span>. A particular MDP may have multiple distinct optimal policies. Because of the<span>&nbsp;</span><a title="Markov property" href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>, it can be shown that the optimal policy is a function of the current state, as assumed above.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Simulator_models">Simulator models</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Simulator models" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=3"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In many cases, it is difficult to represent the transition probability distributions,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c4b70b80a8fc6447e0804ce186459e40faa38db2" alt="{\\displaystyle P_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span>, explicitly. In such cases, a simulator can be used to model the MDP implicitly by providing samples from the transition distributions. One common form of implicit MDP model is an episodic environment simulator that can be started from an initial state and yields a subsequent state and reward every time it receives an action input. In this manner, trajectories of states, actions, and rewards, often called<span>&nbsp;</span><i><dfn>episodes</dfn></i><span>&nbsp;</span>may be produced.</p>\n<p>Another form of simulator is a<span>&nbsp;</span><i><dfn>generative model</dfn></i>, a single step simulator that can generate samples of the next state and reward given any state and action.<sup id="cite_ref-Kearns_Sparse_5-0" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-Kearns_Sparse-5"><span class="cite-bracket">[</span>5<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>(Note that this is a different meaning from the term<span>&nbsp;</span><a title="Generative model" href="https://en.wikipedia.org/wiki/Generative_model">generative model</a><span>&nbsp;</span>in the context of statistical classification.) In<span>&nbsp;</span><a class="mw-redirect" title="Algorithms" href="https://en.wikipedia.org/wiki/Algorithms">algorithms</a><span>&nbsp;</span>that are expressed using<span>&nbsp;</span><a title="Pseudocode" href="https://en.wikipedia.org/wiki/Pseudocode">pseudocode</a>,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle G}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>G</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b" alt="{\\displaystyle G}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is often used to represent a generative model. For example, the expression<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\',r\\gets G(s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>r</mi><mo stretchy="false">←</mo><mi>G</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a0d65fc0bc42952f55b28735cbcc564594e205f" alt="{\\displaystyle s\',r\\gets G(s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>might denote the action of sampling from the generative model where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>are the current state and action, and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle r}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>r</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1ecb613aa2984f0576f70f86650b7c2a132538" alt="{\\displaystyle r}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>are the new state and reward. Compared to an episodic simulator, a generative model has the advantage that it can yield data from any state, not only those encountered in a trajectory.</p>\n<p>These model classes form a hierarchy of information content: an explicit model trivially yields a generative model through sampling from the distributions, and repeated application of a generative model yields an episodic simulator. In the opposite direction, it is only possible to learn approximate models through<span>&nbsp;</span><a title="Regression analysis" href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a>. The type of model available for a particular MDP plays a significant role in determining which solution algorithms are appropriate. For example, the<span>&nbsp;</span><a title="Dynamic programming" href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a><span>&nbsp;</span>algorithms described in the next section require an explicit model, and<span>&nbsp;</span><a title="Monte Carlo tree search" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a><span>&nbsp;</span>requires a generative model (or an episodic simulator that can be copied at any state), whereas most<span>&nbsp;</span><a href="https://en.wikipedia.org/wiki/Markov_decision_process#Reinforcement_learning">reinforcement learning</a><span>&nbsp;</span>algorithms require only an episodic simulator.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Example">Example</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Example" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=4"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<figure class="mw-default-size"><a class="mw-file-description" href="https://en.wikipedia.org/wiki/File:Cartpole.gif"><img class="mw-file-element" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Cartpole.gif/250px-Cartpole.gif" width="250" height="116" data-file-width="600" data-file-height="279" loading="lazy"></a>\n<figcaption>Pole Balancing example (rendering of the environment from the<span>&nbsp;</span><a class="new" title="Open AI gym benchmark (page does not exist)" href="https://en.wikipedia.org/w/index.php?title=Open_AI_gym_benchmark&amp;action=edit&amp;redlink=1">Open AI gym benchmark</a>)</figcaption>\n</figure>\n<p>An example of MDP is the Pole-Balancing model, which comes from classic control theory.</p>\n<p>In this example, we have</p>\n<ul>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>S</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" alt="{\\displaystyle S}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the set of ordered tuples<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (\\theta ,{\\dot {\\theta }},x,{\\dot {x}})\\subset \\mathbb {R} ^{4}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>θ</mi><mo>˙</mo></mover></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo>˙</mo></mover></mrow></mrow><mo stretchy="false">)</mo><mo>⊂</mo><msup><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mn>4</mn></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ff77cda55aebe374aa433ae56006a84ee220426" alt="{\\displaystyle (\\theta ,{\\dot {\\theta }},x,{\\dot {x}})\\subset \\mathbb {R} ^{4}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>given by pole angle, angular velocity, position of the cart and its speed.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle A}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>A</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" alt="{\\displaystyle A}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\{-1,1\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo fence="false" stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo fence="false" stretchy="false">}</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c0ffb8c7a09dad8456eee3669ee9a7e462fe3c34" alt="{\\displaystyle \\{-1,1\\}}" aria-hidden="true" loading="lazy"></span>, corresponding to applying a force on the left (right) on the cart.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c4b70b80a8fc6447e0804ce186459e40faa38db2" alt="{\\displaystyle P_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the transition of the system, which in this case is going to be deterministic and driven by the laws of mechanics.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f842d4a4b1340e194d8014d63163e5e27f94215" alt="{\\displaystyle R_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/92d98b82a3778f043108d4e20960a9193df57cbf" alt="{\\displaystyle 1}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>if the pole is up after the transition, zero otherwise. Therefore, this function only depend on<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>in this specific case.</li>\n</ul>\n<div class="mw-heading mw-heading2">\n<h2 id="Algorithms">Algorithms</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Algorithms" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=5"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as<span>&nbsp;</span><a title="Dynamic programming" href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a>. The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using<span>&nbsp;</span><a title="Function approximation" href="https://en.wikipedia.org/wiki/Function_approximation">function approximation</a>. Also, some processes with countably infinite state and action spaces can be<span>&nbsp;</span><i>exactly</i><span>&nbsp;</span>reduced to ones with finite state and action spaces.<sup id="cite_ref-Wrobel_1984_6-0" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-Wrobel_1984-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup></p>\n<p>The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state:<span>&nbsp;</span><i>value</i><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" alt="{\\displaystyle V}" aria-hidden="true" loading="lazy"></span>, which contains real values, and<span>&nbsp;</span><i>policy</i><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span>, which contains actions. At the end of the algorithm,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>will contain the solution and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b41da2ecbfe4c93303836bb1fe5adb038d50a91" alt="{\\displaystyle V(s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>will contain the discounted sum of the rewards to be earned (on average) by following that solution from state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>.</p>\n<p>The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values.</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V(s):=\\sum _{s\'}P_{\\pi (s)}(s,s\')\\left(R_{\\pi (s)}(s,s\')+\\gamma V(s\')\\right)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>:=</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo>′</mo></msup></mrow></munder><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mrow><mo>(</mo><mrow><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9774acab787cd166e43d70ec03797b73d5e4469e" alt="{\\displaystyle V(s):=\\sum _{s\'}P_{\\pi (s)}(s,s\')\\left(R_{\\pi (s)}(s,s\')+\\gamma V(s\')\\right)}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s):=\\operatorname {argmax} _{a}\\left\\{\\sum _{s\'}P_{a}(s,s\')\\left(R_{a}(s,s\')+\\gamma V(s\')\\right)\\right\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>:=</mo><msub><mi>argmax</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo>⁡</mo><mrow><mo>{</mo><mrow><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo>′</mo></msup></mrow></munder><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mrow><mo>(</mo><mrow><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b18dcfd26569fc8063674d4af7151ed013f299c4" alt="{\\displaystyle \\pi (s):=\\operatorname {argmax} _{a}\\left\\{\\sum _{s\'}P_{a}(s,s\')\\left(R_{a}(s,s\')+\\gamma V(s\')\\right)\\right\\}}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution.<sup id="cite_ref-7" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Notable_variants">Notable variants</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Notable variants" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=6"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="mw-heading mw-heading4">\n<h4 id="Value_iteration">Value iteration</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Value iteration" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=7"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In value iteration (<a href="https://en.wikipedia.org/wiki/Markov_decision_process#CITEREFBellman1957">Bellman 1957</a>), which is also called<span>&nbsp;</span><a title="Backward induction" href="https://en.wikipedia.org/wiki/Backward_induction">backward induction</a>, the<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>function is not used; instead, the value of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is calculated within<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b41da2ecbfe4c93303836bb1fe5adb038d50a91" alt="{\\displaystyle V(s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>whenever it is needed. Substituting the calculation of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>into the calculation of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b41da2ecbfe4c93303836bb1fe5adb038d50a91" alt="{\\displaystyle V(s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>gives the combined step<sup class="noprint Inline-Template">[<i><a title="Wikipedia:Please clarify" href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify"><span title="The derivation of the substituion is needed (July 2018)">further explanation needed</span></a></i>]</sup>:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V_{i+1}(s):=\\max _{a}\\left\\{\\sum _{s\'}P_{a}(s,s\')\\left(R_{a}(s,s\')+\\gamma V_{i}(s\')\\right)\\right\\},}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>:=</mo><munder><mo form="prefix" movablelimits="true">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></munder><mrow><mo>{</mo><mrow><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo>′</mo></msup></mrow></munder><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mrow><mo>(</mo><mrow><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></mrow><mo>}</mo></mrow><mo>,</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86300b0587cbd9e7fc8b4c463f86a7f92861e8bc" alt="{\\displaystyle V_{i+1}(s):=\\max _{a}\\left\\{\\sum _{s\'}P_{a}(s,s\')\\left(R_{a}(s,s\')+\\gamma V_{i}(s\')\\right)\\right\\},}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle i}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>i</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" alt="{\\displaystyle i}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the iteration number. Value iteration starts at<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle i=0}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>i</mi><mo>=</mo><mn>0</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/31a682d568ee6a5fe51d76423186057f625ada5c" alt="{\\displaystyle i=0}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V_{0}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ae15ff9b845587dc4e1816f59c3fed0e71a132f" alt="{\\displaystyle V_{0}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>as a guess of the<span>&nbsp;</span><a title="Value function" href="https://en.wikipedia.org/wiki/Value_function">value function</a>. It then iterates, repeatedly computing<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V_{i+1}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ff121e2116dca444b28b337c30076f0bc3cb4daa" alt="{\\displaystyle V_{i+1}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>for all states<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>, until<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" alt="{\\displaystyle V}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>converges with the left-hand side equal to the right-hand side (which is the "<a title="Bellman equation" href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>" for this problem<sup class="noprint Inline-Template">[<i><a title="Wikipedia:Please clarify" href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify"><span title="The text near this tag may need clarification or removal of jargon. (January 2018)">clarification needed</span></a></i>]</sup>).<span>&nbsp;</span><a title="Lloyd Shapley" href="https://en.wikipedia.org/wiki/Lloyd_Shapley">Lloyd Shapley</a>\'s 1953 paper on<span>&nbsp;</span><a class="mw-redirect" title="Stochastic games" href="https://en.wikipedia.org/wiki/Stochastic_games">stochastic games</a><span>&nbsp;</span>included as a special case the value iteration method for MDPs,<sup id="cite_ref-8" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-8"><span class="cite-bracket">[</span>8<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>but this was recognized only later on.<sup id="cite_ref-9" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-9"><span class="cite-bracket">[</span>9<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading4">\n<h4 id="Policy_iteration">Policy iteration</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Policy iteration" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=8"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In policy iteration (<a href="https://en.wikipedia.org/wiki/Markov_decision_process#CITEREFHoward1960">Howard 1960</a>), step one is performed once, and then step two is performed once, then both are repeated until policy converges. Then step one is again performed once and so on. (Policy iteration was invented by Howard to optimize<span>&nbsp;</span><a title="Sears" href="https://en.wikipedia.org/wiki/Sears">Sears</a><span>&nbsp;</span>catalogue mailing, which he had been optimizing using value iteration.<sup id="cite_ref-10" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-10"><span class="cite-bracket">[</span>10<span class="cite-bracket">]</span></a></sup>)</p>\n<p>Instead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s=s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi><mo>=</mo><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13204863dd8c3e7375c6d96b55b9175c42862ef8" alt="{\\displaystyle s=s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>in the step two equation.<sup class="noprint Inline-Template">[<i><a title="Wikipedia:Please clarify" href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify"><span title="The text near this tag may need clarification or removal of jargon. (January 2018)">clarification needed</span></a></i>]</sup><span>&nbsp;</span>Thus, repeating step two to convergence can be interpreted as solving the linear equations by<span>&nbsp;</span><a title="Relaxation (iterative method)" href="https://en.wikipedia.org/wiki/Relaxation_(iterative_method)">relaxation</a>.</p>\n<p>This variant has the advantage that there is a definite stopping condition: when the array<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>does not change in the course of applying step 1 to all states, the algorithm is completed.</p>\n<p>Policy iteration is usually slower than value iteration for a large number of possible states.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="Modified_policy_iteration">Modified policy iteration</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Modified policy iteration" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=9"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In modified policy iteration (<a href="https://en.wikipedia.org/wiki/Markov_decision_process#CITEREFvan_Nunen1976">van Nunen 1976</a>;<span>&nbsp;</span><a href="https://en.wikipedia.org/wiki/Markov_decision_process#CITEREFPutermanShin1978">Puterman &amp; Shin 1978</a>), step one is performed once, and then step two is repeated several times.<sup id="cite_ref-11" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-11"><span class="cite-bracket">[</span>11<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-12" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Then step one is again performed once and so on.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="Prioritized_sweeping">Prioritized sweeping</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Prioritized sweeping" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=10"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>V</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" alt="{\\displaystyle V}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>or<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm).</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Computational_complexity">Computational complexity</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Computational complexity" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=11"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Algorithms for finding optimal policies with<span>&nbsp;</span><a title="Time complexity" href="https://en.wikipedia.org/wiki/Time_complexity">time complexity</a><span>&nbsp;</span>polynomial in the size of the problem representation exist for finite MDPs. Thus,<span>&nbsp;</span><a title="Decision problem" href="https://en.wikipedia.org/wiki/Decision_problem">decision problems</a><span>&nbsp;</span>based on MDPs are in computational<span>&nbsp;</span><a title="Complexity class" href="https://en.wikipedia.org/wiki/Complexity_class">complexity class</a><span>&nbsp;</span><a title="P (complexity)" href="https://en.wikipedia.org/wiki/P_(complexity)">P</a>.<sup id="cite_ref-13" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>However, due to the<span>&nbsp;</span><a title="Curse of dimensionality" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>, the size of the problem representation is often exponential in the number of state and action variables, limiting exact solution techniques to problems that have a compact representation. In practice, online planning techniques such as<span>&nbsp;</span><a title="Monte Carlo tree search" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a><span>&nbsp;</span>can find useful solutions in larger problems, and, in theory, it is possible to construct online planning algorithms that can find an arbitrarily near-optimal policy with no computational complexity dependence on the size of the state space.<sup id="cite_ref-14" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Extensions_and_generalizations">Extensions and generalizations</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Extensions and generalizations" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=12"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>A Markov decision process is a<span>&nbsp;</span><a title="Stochastic game" href="https://en.wikipedia.org/wiki/Stochastic_game">stochastic game</a><span>&nbsp;</span>with only one player.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Partial_observability">Partial observability</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Partial observability" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=13"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="hatnote navigation-not-searchable" role="note">Main article:<span>&nbsp;</span><a title="Partially observable Markov decision process" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">Partially observable Markov decision process</a></div>\n<p>The solution above assumes that the state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is known when action is to be taken; otherwise<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Constrained_Markov_decision_processes">Constrained Markov decision processes</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Constrained Markov decision processes" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=14"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Constrained Markov decision processes (CMDPS) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs.<sup id="cite_ref-15" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-15"><span class="cite-bracket">[</span>15<span class="cite-bracket">]</span></a></sup></p>\n<ul>\n<li>There are multiple costs incurred after applying an action instead of one.</li>\n<li>CMDPs are solved with<span>&nbsp;</span><a title="Linear programming" href="https://en.wikipedia.org/wiki/Linear_programming">linear programs</a><span>&nbsp;</span>only, and<span>&nbsp;</span><a title="Dynamic programming" href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a><span>&nbsp;</span>does not work.</li>\n<li>The final policy depends on the starting state.</li>\n</ul>\n<p>The method of Lagrange multipliers applies to CMDPs. Many Lagrangian-based algorithms have been developed.</p>\n<ul>\n<li>Natural policy gradient primal-dual method.<sup id="cite_ref-16" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-16"><span class="cite-bracket">[</span>16<span class="cite-bracket">]</span></a></sup></li>\n</ul>\n<p>There are a number of applications for CMDPs. It has recently been used in<span>&nbsp;</span><a title="Motion planning" href="https://en.wikipedia.org/wiki/Motion_planning">motion planning</a><span>&nbsp;</span>scenarios in robotics.<sup id="cite_ref-17" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-17"><span class="cite-bracket">[</span>17<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Continuous-time_Markov_decision_process">Continuous-time Markov decision process</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Continuous-time Markov decision process" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=15"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for<span>&nbsp;</span><strong>continuous-time Markov decision processes</strong>, decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision-making process for a system that has<span>&nbsp;</span><a class="mw-redirect" title="Continuous time" href="https://en.wikipedia.org/wiki/Continuous_time">continuous dynamics</a>, i.e.,&nbsp;the system dynamics is defined by<span>&nbsp;</span><a title="Ordinary differential equation" href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equations</a><span>&nbsp;</span>(ODEs). These kind of applications raise in<span>&nbsp;</span><a class="mw-redirect" title="Queueing system" href="https://en.wikipedia.org/wiki/Queueing_system">queueing systems</a>, epidemic processes, and<span>&nbsp;</span><a title="Population process" href="https://en.wikipedia.org/wiki/Population_process">population processes</a>.</p>\n<p>Like the discrete-time Markov decision processes, in continuous-time Markov decision processes the agent aims at finding the optimal<span>&nbsp;</span><i>policy</i><span>&nbsp;</span>which could maximize the expected cumulated reward. The only difference with the standard case stays in the fact that, due to the continuous nature of the time variable, the sum is replaced by an integral:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\max \\operatorname {E} _{\\pi }\\left[\\left.\\int _{0}^{\\infty }\\gamma ^{t}r(s(t),\\pi (s(t)))\\,dt\\;\\right|s_{0}\\right]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo form="prefix" movablelimits="true">max</mo><msub><mi mathvariant="normal">E</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msub><mo>⁡</mo><mrow><mo>[</mo><mrow><mrow><mo fence="true" stretchy="true" symmetric="true"></mo><mrow><msubsup><mo>∫</mo><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></msubsup><msup><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup><mi>r</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>,</mo><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace></mspace><mi>d</mi><mi>t</mi><mspace></mspace></mrow><mo>|</mo></mrow><msub><mi>s</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub></mrow><mo>]</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ac99ceb438659993443fb0090c0e3d18ae787ab" alt="{\\displaystyle \\max \\operatorname {E} _{\\pi }\\left[\\left.\\int _{0}^{\\infty }\\gamma ^{t}r(s(t),\\pi (s(t)))\\,dt\\;\\right|s_{0}\\right]}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 0\\leq \\gamma <1.}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn><mo>≤</mo><mi>γ</mi><mo>&lt;</mo><mn>1.</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f607217c39d955271ad1a792f1a3dd98357259a9" alt="{\\displaystyle 0\\leq \\gamma <1.}" aria-hidden="true" loading="lazy"></span></p>\n<div class="mw-heading mw-heading4">\n<h4 id="Discrete_space:_Linear_programming_formulation">Discrete space: Linear programming formulation</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Discrete space: Linear programming formulation" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=16"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>If the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an<span>&nbsp;</span><a title="Ergodicity" href="https://en.wikipedia.org/wiki/Ergodicity">ergodic</a><span>&nbsp;</span>continuous-time Markov chain under a stationary<span>&nbsp;</span><a title="Policy" href="https://en.wikipedia.org/wiki/Policy">policy</a>. Under this assumption, although the decision maker can make a decision at any time in the current state, there is no benefit in taking multiple actions. It is better to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,<sup id="cite_ref-18" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-18"><span class="cite-bracket">[</span>18<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>if our optimal value function<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5910e6a94f4f7ee2ee85ceed9dacef3eff7a6242" alt="{\\displaystyle V^{*}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is independent of state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle i}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>i</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" alt="{\\displaystyle i}" aria-hidden="true" loading="lazy"></span>, we will have the following inequality:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle g\\geq R(i,a)+\\sum _{j\\in S}q(j\\mid i,a)h(j)\\quad \\forall i\\in S{\\text{ and }}a\\in A(i)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>g</mi><mo>≥</mo><mi>R</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>q</mi><mo stretchy="false">(</mo><mi>j</mi><mo>∣</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>h</mi><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo><mspace></mspace><mi mathvariant="normal">∀</mi><mi>i</mi><mo>∈</mo><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;and&nbsp;</mtext></mrow><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/aa97af42b532c2bc0afcaf087feda3fd3dca6051" alt="{\\displaystyle g\\geq R(i,a)+\\sum _{j\\in S}q(j\\mid i,a)h(j)\\quad \\forall i\\in S{\\text{ and }}a\\in A(i)}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>If there exists a function<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle h}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>h</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b26be3e694314bc90c3215047e4a2010c6ee184a" alt="{\\displaystyle h}" aria-hidden="true" loading="lazy"></span>, then<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\bar {V}}^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>V</mi><mo stretchy="false">¯</mo></mover></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/469a7ee0366792acb671e6eb13fc755a09b15c1c" alt="{\\displaystyle {\\bar {V}}^{*}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>will be the smallest<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle g}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>g</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3556280e66fe2c0d0140df20935a6f057381d77" alt="{\\displaystyle g}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>satisfying the above equation. In order to find<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\bar {V}}^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mover><mi>V</mi><mo stretchy="false">¯</mo></mover></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/469a7ee0366792acb671e6eb13fc755a09b15c1c" alt="{\\displaystyle {\\bar {V}}^{*}}" aria-hidden="true" loading="lazy"></span>, we could use the following linear programming model:</p>\n<ul>\n<li>Primal linear program(P-LP)</li>\n</ul>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\begin{aligned}{\\text{Minimize}}\\quad &amp;g\\\\{\\text{s.t}}\\quad &amp;g-\\sum _{j\\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\\forall i\\in S,\\,a\\in A(i)\\end{aligned}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>Minimize</mtext></mrow><mspace></mspace></mtd><mtd><mi>g</mi></mtd></mtr><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>s.t</mtext></mrow><mspace></mspace></mtd><mtd><mi>g</mi><mo>−</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>j</mi><mo>∈</mo><mi>S</mi></mrow></munder><mi>q</mi><mo stretchy="false">(</mo><mi>j</mi><mo>∣</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>h</mi><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo><mo>≥</mo><mi>R</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mspace></mspace><mspace></mspace><mi mathvariant="normal">∀</mi><mi>i</mi><mo>∈</mo><mi>S</mi><mo>,</mo><mspace></mspace><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec3d7ccf2048eea9a7a1dc2bf8fc4b2921e22f6b" alt="{\\displaystyle {\\begin{aligned}{\\text{Minimize}}\\quad &amp;g\\\\{\\text{s.t}}\\quad &amp;g-\\sum _{j\\in S}q(j\\mid i,a)h(j)\\geq R(i,a)\\,\\,\\forall i\\in S,\\,a\\in A(i)\\end{aligned}}}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<ul>\n<li>Dual linear program(D-LP)</li>\n</ul>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\begin{aligned}{\\text{Maximize}}&amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\\\{\\text{s.t.}}&amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}q(j\\mid i,a)y(i,a)=0\\quad \\forall j\\in S,\\\\\u0026amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}y(i,a)=1,\\\\\u0026amp;y(i,a)\\geq 0\\qquad \\forall a\\in A(i){\\text{ and }}\\forall i\\in S\\end{aligned}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>Maximize</mtext></mrow></mtd><mtd><mi></mi><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>R</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>s.t.</mtext></mrow></mtd><mtd><mi></mi><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>q</mi><mo stretchy="false">(</mo><mi>j</mi><mo>∣</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mspace></mspace><mi mathvariant="normal">∀</mi><mi>j</mi><mo>∈</mo><mi>S</mi><mo>,</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi></mi><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>,</mo></mtd></mtr><mtr><mtd></mtd><mtd><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>≥</mo><mn>0</mn><mspace></mspace><mi mathvariant="normal">∀</mi><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;and&nbsp;</mtext></mrow><mi mathvariant="normal">∀</mi><mi>i</mi><mo>∈</mo><mi>S</mi></mtd></mtr></mtable></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/71a8c73e8ee8c269c14ca53548b8dfeecbbb9bf6" alt="{\\displaystyle {\\begin{aligned}{\\text{Maximize}}&amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\\\{\\text{s.t.}}&amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}q(j\\mid i,a)y(i,a)=0\\quad \\forall j\\in S,\\\\\u0026amp;\\sum _{i\\in S}\\sum _{a\\in A(i)}y(i,a)=1,\\\\\u0026amp;y(i,a)\\geq 0\\qquad \\forall a\\in A(i){\\text{ and }}\\forall i\\in S\\end{aligned}}}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle y(i,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/96f0bcd6206bb46d5f862ef6c867c14f7ef640ef" alt="{\\displaystyle y(i,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a feasible solution to the D-LP if<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle y(i,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/96f0bcd6206bb46d5f862ef6c867c14f7ef640ef" alt="{\\displaystyle y(i,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is nonnative and satisfied the constraints in the D-LP problem. A feasible solution<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle y^{*}(i,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22e922881d43c19785fcc31d9733c1f444b8b603" alt="{\\displaystyle y^{*}(i,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to the D-LP is said to be an optimal solution if</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\begin{aligned}\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y^{*}(i,a)\\geq \\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\end{aligned}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>R</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><msup><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>≥</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo>∈</mo><mi>A</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>R</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f84bb5a3d2375caadd417fa57466231c9448e0af" alt="{\\displaystyle {\\begin{aligned}\\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y^{*}(i,a)\\geq \\sum _{i\\in S}\\sum _{a\\in A(i)}R(i,a)y(i,a)\\end{aligned}}}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>for all feasible solution<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle y(i,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/96f0bcd6206bb46d5f862ef6c867c14f7ef640ef" alt="{\\displaystyle y(i,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to the D-LP. Once we have found the optimal solution<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle y^{*}(i,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/22e922881d43c19785fcc31d9733c1f444b8b603" alt="{\\displaystyle y^{*}(i,a)}" aria-hidden="true" loading="lazy"></span>, we can use it to establish the optimal policies.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="Continuous_space:_Hamilton–Jacobi–Bellman_equation"><span id="Continuous_space:_Hamilton.E2.80.93Jacobi.E2.80.93Bellman_equation"></span>Continuous space: Hamilton–Jacobi–Bellman equation</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Continuous space: Hamilton–Jacobi–Bellman equation" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=17"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving<span>&nbsp;</span><a title="Hamilton–Jacobi–Bellman equation" href="https://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation">Hamilton–Jacobi–Bellman (HJB) partial differential equation</a>. In order to discuss the HJB equation, we need to reformulate our problem</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\begin{aligned}V(s(0),0)={}&amp;\\max _{a(t)=\\pi (s(t))}\\int _{0}^{T}r(s(t),a(t))\\,dt+D[s(T)]\\\\{\\text{s.t.}}\\quad &amp;{\\frac {ds(t)}{dt}}=f[t,s(t),a(t)]\\end{aligned}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mtr><mtd><mi>V</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>,</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-ORD"></mrow></mtd><mtd><mi></mi><munder><mo form="prefix" movablelimits="true">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></munder><msubsup><mo>∫</mo><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><mi>r</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>,</mo><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mspace></mspace><mi>d</mi><mi>t</mi><mo>+</mo><mi>D</mi><mo stretchy="false">[</mo><mi>s</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>s.t.</mtext></mrow><mspace></mspace></mtd><mtd><mrow class="MJX-TeXAtom-ORD"><mfrac><mrow><mi>d</mi><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac></mrow><mo>=</mo><mi>f</mi><mo stretchy="false">[</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>,</mo><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3073614d5275e926989efa835620479d6117d8" alt="{\\displaystyle {\\begin{aligned}V(s(0),0)={}&amp;\\max _{a(t)=\\pi (s(t))}\\int _{0}^{T}r(s(t),a(t))\\,dt+D[s(T)]\\\\{\\text{s.t.}}\\quad &amp;{\\frac {ds(t)}{dt}}=f[t,s(t),a(t)]\\end{aligned}}}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle D(\\cdot )}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>D</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5d01ee4dfaa3874c82cd231a3bd133ea2b27ad50" alt="{\\displaystyle D(\\cdot )}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the terminal reward function,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s(t)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c484de351ba40ccb9a5ad522c29c1aac5686c0df" alt="{\\displaystyle s(t)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the system state vector,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a(t)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bb7931a26b0d360eaf90aa45247d2de5c984d5d8" alt="{\\displaystyle a(t)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the system control vector we try to find.<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle f(\\cdot )}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>f</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6ec4adac182c93a76f0238a839ebb10b124e54c2" alt="{\\displaystyle f(\\cdot )}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>shows how the state vector changes over time. The Hamilton–Jacobi–Bellman equation is as follows:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 0=\\max _{u}(r(t,s,a)+{\\frac {\\partial V(t,s)}{\\partial x}}f(t,s,a))}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn><mo>=</mo><munder><mo form="prefix" movablelimits="true">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>u</mi></mrow></munder><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>V</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi></mrow></mfrac></mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eefff3d70e56d0317facf0ecefb2106437cca7a0" alt="{\\displaystyle 0=\\max _{u}(r(t,s,a)+{\\frac {\\partial V(t,s)}{\\partial x}}f(t,s,a))}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>We could solve the equation to find the optimal control<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a(t)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bb7931a26b0d360eaf90aa45247d2de5c984d5d8" alt="{\\displaystyle a(t)}" aria-hidden="true" loading="lazy"></span>, which could give us the optimal<span>&nbsp;</span><a title="Value function" href="https://en.wikipedia.org/wiki/Value_function">value function</a><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5910e6a94f4f7ee2ee85ceed9dacef3eff7a6242" alt="{\\displaystyle V^{*}}" aria-hidden="true" loading="lazy"></span></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Reinforcement_learning">Reinforcement learning</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=18"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="hatnote navigation-not-searchable" role="note">Main article:<span>&nbsp;</span><a title="Reinforcement learning" href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning</a></div>\n<p><a title="Reinforcement learning" href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning</a><span>&nbsp;</span>is an interdisciplinary area of<span>&nbsp;</span><a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Optimal control" href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a><span>&nbsp;</span>that has, as main objective, finding an approximately optimal policy for MDPs where transition probabilities and rewards are unknown.<sup id="cite_ref-19" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-19"><span class="cite-bracket">[</span>19<span class="cite-bracket">]</span></a></sup></p>\n<p>Reinforcement learning can solve Markov-Decision processes without explicit specification of the transition probabilities which are instead needed to perform policy iteration. In this setting, transition probabilities and rewards must be learned from experience, i.e. by letting an agent interact with the MDP for a given number of steps. Both on a theoretical and on a practical level, effort is put in maximizing the sample efficiency, i.e. minimimizing the number of samples needed to learn a policy whose performance is<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\varepsilon -}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ε</mi><mo>−</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c98143e838e1b3767e8727ca4e08d79124aadb71" alt="{\\displaystyle \\varepsilon -}" aria-hidden="true" loading="lazy"></span>close to the optimal one (due to the stochastic nature of the process, learning the optimal policy with a finite number of samples is, in general, impossible).</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Reinforcement_Learning_for_discrete_MDPs">Reinforcement Learning for discrete MDPs</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Reinforcement Learning for discrete MDPs" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=19"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>For the purpose of this section, it is useful to define a further function, which corresponds to taking the action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and then continuing optimally (or according to whatever policy one currently has):</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\ Q(s,a)=\\sum _{s\'}P_{a}(s,s\')(R_{a}(s,s\')+\\gamma V(s\')).\\ }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mtext>&nbsp;</mtext><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo>′</mo></msup></mrow></munder><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>.</mo><mtext>&nbsp;</mtext></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/63b502aafbe6ea1585231222ea3783f40f0808a9" alt="{\\displaystyle \\ Q(s,a)=\\sum _{s\'}P_{a}(s,s\')(R_{a}(s,s\')+\\gamma V(s\')).\\ }" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>While this function is also unknown, experience during learning is based on<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e483c36a133514a0cd6d604f5cce56d2fd4cae9" alt="{\\displaystyle (s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>pairs (together with the outcome<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span>; that is, "I was in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and I tried doing<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>happened"). Thus, one has an array<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>Q</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8752c7023b4b3286800fe3238271bbca681219ed" alt="{\\displaystyle Q}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and uses experience to update it directly. This is known as<span>&nbsp;</span><a title="Q-learning" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Other_scopes">Other scopes</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Other scopes" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=20"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="mw-heading mw-heading3">\n<h3 id="Learning_automata">Learning automata</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Learning automata" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=21"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="hatnote navigation-not-searchable" role="note">Main article:<span>&nbsp;</span><a class="mw-redirect" title="Learning automata" href="https://en.wikipedia.org/wiki/Learning_automata">Learning automata</a></div>\n<p>Another application of MDP process in<span>&nbsp;</span><a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a><span>&nbsp;</span>theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail<span>&nbsp;</span><strong>learning automata</strong><span>&nbsp;</span>paper is surveyed by<span>&nbsp;</span><a title="Kumpati S. Narendra" href="https://en.wikipedia.org/wiki/Kumpati_S._Narendra">Narendra</a><span>&nbsp;</span>and Thathachar (1974), which were originally described explicitly as<span>&nbsp;</span><a class="mw-redirect" title="Finite-state automata" href="https://en.wikipedia.org/wiki/Finite-state_automata">finite-state automata</a>.<sup id="cite_ref-20" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-20"><span class="cite-bracket">[</span>20<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence.<sup id="cite_ref-NarendraEtAl1989_21-0" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-NarendraEtAl1989-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup></p>\n<p>In learning automata theory,<span>&nbsp;</span><strong>a stochastic automaton</strong><span>&nbsp;</span>consists of:</p>\n<ul>\n<li>a set<span>&nbsp;</span><i>x</i><span>&nbsp;</span>of possible inputs,</li>\n<li>a set Φ = { Φ<sub>1</sub>, ..., Φ<sub><i>s</i></sub><span>&nbsp;</span>} of possible internal states,</li>\n<li>a set α = { α<sub>1</sub>, ..., α<sub><i>r</i></sub><span>&nbsp;</span>} of possible outputs, or actions, with<span>&nbsp;</span><i>r</i><span>&nbsp;</span>≤<span>&nbsp;</span><i>s</i>,</li>\n<li>an initial state probability vector<span>&nbsp;</span><i>p</i>(0) = ≪<span>&nbsp;</span><i>p</i><sub>1</sub>(0), ...,<span>&nbsp;</span><i>p<sub>s</sub></i>(0) ≫,</li>\n<li>a<span>&nbsp;</span><a title="Computable function" href="https://en.wikipedia.org/wiki/Computable_function">computable function</a><span>&nbsp;</span><i>A</i><span>&nbsp;</span>which after each time step<span>&nbsp;</span><i>t</i><span>&nbsp;</span>generates<span>&nbsp;</span><i>p</i>(<i>t</i><span>&nbsp;</span>+ 1) from<span>&nbsp;</span><i>p</i>(<i>t</i>), the current input, and the current state, and</li>\n<li>a function<span>&nbsp;</span><i>G</i>: Φ → α which generates the output at each time step.</li>\n</ul>\n<p>The states of such an automaton correspond to the states of a "discrete-state discrete-parameter<span>&nbsp;</span><a class="mw-redirect" title="Markov process" href="https://en.wikipedia.org/wiki/Markov_process">Markov process</a>".<sup id="cite_ref-FOOTNOTENarendraThathachar1974p.325_left_22-0" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-FOOTNOTENarendraThathachar1974p.325_left-22"><span class="cite-bracket">[</span>22<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>At each time step<span>&nbsp;</span><i>t</i><span>&nbsp;</span>= 0,1,2,3,..., the automaton reads an input from its environment, updates P(<i>t</i>) to P(<i>t</i><span>&nbsp;</span>+ 1) by<span>&nbsp;</span><i>A</i>, randomly chooses a successor state according to the probabilities P(<i>t</i><span>&nbsp;</span>+ 1) and outputs the corresponding action. The automaton\'s environment, in turn, reads the action and sends the next input to the automaton.<sup id="cite_ref-NarendraEtAl1989_21-1" class="reference"><a href="https://en.wikipedia.org/wiki/Markov_decision_process#cite_note-NarendraEtAl1989-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Category_theoretic_interpretation">Category theoretic interpretation</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Category theoretic interpretation" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=22"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Other than the rewards, a Markov decision process<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (S,A,P)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>S</mi><mo>,</mo><mi>A</mi><mo>,</mo><mi>P</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b1e1ec529b95a188c2cd2b238c1428551c2a913b" alt="{\\displaystyle (S,A,P)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>can be understood in terms of<span>&nbsp;</span><a title="Category theory" href="https://en.wikipedia.org/wiki/Category_theory">Category theory</a>. Namely, let<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\mathcal {A}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/280ae03440942ab348c2ca9b8db6b56ffa9618f8" alt="{\\displaystyle {\\mathcal {A}}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>denote the<span>&nbsp;</span><a title="Free monoid" href="https://en.wikipedia.org/wiki/Free_monoid">free monoid</a><span>&nbsp;</span>with generating set<span>&nbsp;</span><i>A</i>. Let<span>&nbsp;</span><strong>Dist</strong><span>&nbsp;</span>denote the<span>&nbsp;</span><a title="Kleisli category" href="https://en.wikipedia.org/wiki/Kleisli_category">Kleisli category</a><span>&nbsp;</span>of the<span>&nbsp;</span><a class="external text" href="http://ncatlab.org/nlab/show/Giry+monad">Giry monad</a>. Then a functor<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\mathcal {A}}\\to \\mathbf {Dist} }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></mrow><mo stretchy="false">→</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">D</mi><mi mathvariant="bold">i</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">t</mi></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b0ec596d0f698d2330a30ddbc988ba8bd7228e2" alt="{\\displaystyle {\\mathcal {A}}\\to \\mathbf {Dist} }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>encodes both the set<span>&nbsp;</span><i>S</i><span>&nbsp;</span>of states and the probability function<span>&nbsp;</span><i>P</i>.</p>\n<p>In this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle ({\\mathcal {C}},F:{\\mathcal {C}}\\to \\mathbf {Dist} )}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">C</mi></mrow></mrow><mo>,</mo><mi>F</mi><mo>:</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">C</mi></mrow></mrow><mo stretchy="false">→</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="bold">D</mi><mi mathvariant="bold">i</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">t</mi></mrow><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d2c1b727a565d5cca3453563ed3296658b955108" alt="{\\displaystyle ({\\mathcal {C}},F:{\\mathcal {C}}\\to \\mathbf {Dist} )}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>a<span>&nbsp;</span><i>context-dependent Markov decision process</i>, because moving from one object to another in<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\mathcal {C}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">C</mi></mrow></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e7b3edab7022ca9e2976651bc59c489513ee9019" alt="{\\displaystyle {\\mathcal {C}}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>changes the set of available actions and the set of possible states.<sup class="noprint Inline-Template Template-Fact">[<i><a title="Wikipedia:Citation needed" href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed"><span title="No reference is provided (December 2020)">citation needed</span></a></i>]</sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Alternative_notations">Alternative notations</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Alternative notations" href="https://en.wikipedia.org/w/index.php?title=Markov_decision_process&amp;action=edit&amp;section=23"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor<span>&nbsp;</span><span class="texhtml mvar">β</span><span>&nbsp;</span>or<span>&nbsp;</span><span class="texhtml mvar">γ</span>, while the other focuses on minimization problems from engineering and navigation<sup class="noprint Inline-Template Template-Fact">[<i><a title="Wikipedia:Citation needed" href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed"><span title="This claim needs references to reliable sources. (December 2019)">citation needed</span></a></i>]</sup>, using the terms control, cost, cost-to-go, and calling the discount factor<span>&nbsp;</span><span class="texhtml mvar">α</span>. In addition, the notation for the transition probability varies.</p>\n<table class="wikitable">\n<tbody>\n<tr>\n<th>in this article</th>\n<th>alternative</th>\n<th>comment</th>\n</tr>\n<tr>\n<td>action<span>&nbsp;</span><span class="texhtml mvar">a</span></td>\n<td>control<span>&nbsp;</span><span class="texhtml mvar">u</span></td>\n<td></td>\n</tr>\n<tr>\n<td>reward<span>&nbsp;</span><span class="texhtml mvar">R</span></td>\n<td>cost<span>&nbsp;</span><span class="texhtml mvar">g</span></td>\n<td><span class="texhtml mvar">g</span><span>&nbsp;</span>is the negative of<span>&nbsp;</span><span class="texhtml mvar">R</span></td>\n</tr>\n<tr>\n<td>value<span>&nbsp;</span><span class="texhtml mvar">V</span></td>\n<td>cost-to-go<span>&nbsp;</span><span class="texhtml mvar">J</span></td>\n<td><span class="texhtml mvar">J</span><span>&nbsp;</span>is the negative of<span>&nbsp;</span><span class="texhtml mvar">V</span></td>\n</tr>\n<tr>\n<td>policy<span>&nbsp;</span><span class="texhtml mvar">π</span></td>\n<td>policy<span>&nbsp;</span><span class="texhtml mvar">μ</span></td>\n<td></td>\n</tr>\n<tr>\n<td>discounting factor<span>&nbsp;</span><span class="texhtml mvar">γ</span></td>\n<td>discounting factor<span>&nbsp;</span><span class="texhtml mvar">α</span></td>\n<td></td>\n</tr>\n<tr>\n<td>transition probability<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c4b70b80a8fc6447e0804ce186459e40faa38db2" alt="{\\displaystyle P_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span></td>\n<td>transition probability<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle p_{ss\'}(a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><msup><mi>s</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/51587860f2ab0a112934c71312c3f2770dae4d1d" alt="{\\displaystyle p_{ss\'}(a)}" aria-hidden="true" loading="lazy"></span></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>In addition, transition probability is sometimes written<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\Pr(s,a,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/298f9c2686072c94dd63f292f1a7fd9decf06bff" alt="{\\displaystyle \\Pr(s,a,s\')}" aria-hidden="true" loading="lazy"></span>,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\Pr(s\'\\mid s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo>∣</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65918837aacf3111afdac4dd2517c6652e4c058a" alt="{\\displaystyle \\Pr(s\'\\mid s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>or, rarely,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle p_{s\'s}(a).}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>p</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>s</mi><mo>′</mo></msup><mi>s</mi></mrow></msub><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mo>.</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e74667f0d3eb085e867e6bc92fb823500a8cfe0b" alt="{\\displaystyle p_{s\'s}(a).}" aria-hidden="true" loading="lazy"></span></p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div>\n<div class="mw-footer-container"><footer id="footer" class="mw-footer">\n<ul id="footer-icons" class="noprint">\n<li id="footer-poweredbyico"></li>\n</ul>\n</footer></div>\n</div>\n</div><script src="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3521935/dp_app.js"></script>'


csp_body = '<link rel="stylesheet" href="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3563198/dp_colors_uit_blue.css"><div class="mw-page-container">\n<div class="mw-page-container-inner">\n<div class="mw-content-container">\n<div id="bodyContent" class="vector-body ve-init-mw-desktopArticleTarget-targetContainer" aria-labelledby="firstHeading" data-mw-ve-target-container="">\n<div id="mw-content-text" class="mw-body-content">\n<div class="mw-content-ltr mw-parser-output" dir="ltr" lang="en">\n<p><strong>Constraint satisfaction problems</strong><span>&nbsp;</span>(<strong>CSPs</strong>) are mathematical questions defined as a set of objects whose<span>&nbsp;</span><a title="State (computer science)" href="https://en.wikipedia.org/wiki/State_(computer_science)">state</a><span>&nbsp;</span>must satisfy a number of<span>&nbsp;</span><a title="Constraint (mathematics)" href="https://en.wikipedia.org/wiki/Constraint_(mathematics)">constraints</a><span>&nbsp;</span>or<span>&nbsp;</span><a title="Limit (mathematics)" href="https://en.wikipedia.org/wiki/Limit_(mathematics)">limitations</a>. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over<span>&nbsp;</span><a title="Variable (mathematics)" href="https://en.wikipedia.org/wiki/Variable_(mathematics)">variables</a>, which is solved by<span>&nbsp;</span><a title="Constraint satisfaction" href="https://en.wikipedia.org/wiki/Constraint_satisfaction">constraint satisfaction</a><span>&nbsp;</span>methods. CSPs are the subject of research in both<span>&nbsp;</span><a title="Artificial intelligence" href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Operations research" href="https://en.wikipedia.org/wiki/Operations_research">operations research</a>, since the regularity in their formulation provides a common basis to analyze and solve problems of many seemingly unrelated families.<span>&nbsp;</span><a title="Complexity of constraint satisfaction" href="https://en.wikipedia.org/wiki/Complexity_of_constraint_satisfaction">CSPs often exhibit high complexity</a>, requiring a combination of<span>&nbsp;</span><a class="mw-redirect" title="Heuristics" href="https://en.wikipedia.org/wiki/Heuristics">heuristics</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Combinatorial search" href="https://en.wikipedia.org/wiki/Combinatorial_search">combinatorial search</a><span>&nbsp;</span>methods to be solved in a reasonable time.<span>&nbsp;</span><a title="Constraint programming" href="https://en.wikipedia.org/wiki/Constraint_programming">Constraint programming</a><span>&nbsp;</span>(CP) is the field of research that specifically focuses on tackling these kinds of problems.<sup id="cite_ref-1" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-2" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-2"><span class="cite-bracket">[</span>2<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Additionally, the<span>&nbsp;</span><a title="Boolean satisfiability problem" href="https://en.wikipedia.org/wiki/Boolean_satisfiability_problem">Boolean satisfiability problem</a><span>&nbsp;</span>(SAT),<span>&nbsp;</span><a title="Satisfiability modulo theories" href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">satisfiability modulo theories</a><span>&nbsp;</span>(SMT),<span>&nbsp;</span><a class="mw-redirect" title="Mixed integer programming" href="https://en.wikipedia.org/wiki/Mixed_integer_programming">mixed integer programming</a><span>&nbsp;</span>(MIP) and<span>&nbsp;</span><a title="Answer set programming" href="https://en.wikipedia.org/wiki/Answer_set_programming">answer set programming</a><span>&nbsp;</span>(ASP) are all fields of research focusing on the resolution of particular forms of the constraint satisfaction problem.</p>\n<p>Examples of problems that can be modeled as a constraint satisfaction problem include:</p>\n<ul>\n<li><a title="Type inference" href="https://en.wikipedia.org/wiki/Type_inference">Type inference</a><sup id="cite_ref-3" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-4" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup></li>\n<li><a title="Eight queens puzzle" href="https://en.wikipedia.org/wiki/Eight_queens_puzzle">Eight queens puzzle</a></li>\n<li><a title="Graph coloring" href="https://en.wikipedia.org/wiki/Graph_coloring">Map coloring problem</a></li>\n<li><a title="Maximum cut" href="https://en.wikipedia.org/wiki/Maximum_cut">Maximum cut problem</a><sup id="cite_ref-5" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-5"><span class="cite-bracket">[</span>5<span class="cite-bracket">]</span></a></sup></li>\n<li><a title="Sudoku" href="https://en.wikipedia.org/wiki/Sudoku">Sudoku</a>,<span>&nbsp;</span><a title="Crossword" href="https://en.wikipedia.org/wiki/Crossword">crosswords</a>,<span>&nbsp;</span><a title="Futoshiki" href="https://en.wikipedia.org/wiki/Futoshiki">futoshiki</a>,<span>&nbsp;</span><a title="Kakuro" href="https://en.wikipedia.org/wiki/Kakuro">Kakuro</a><span>&nbsp;</span>(Cross Sums),<span>&nbsp;</span><a class="mw-redirect" title="Numbrix" href="https://en.wikipedia.org/wiki/Numbrix">Numbrix</a>/<a title="Hidato" href="https://en.wikipedia.org/wiki/Hidato">Hidato</a>,<span>&nbsp;</span><a title="Zebra Puzzle" href="https://en.wikipedia.org/wiki/Zebra_Puzzle">Zebra Puzzle</a>, and many other<span>&nbsp;</span><a title="Logic puzzle" href="https://en.wikipedia.org/wiki/Logic_puzzle">logic puzzles</a></li>\n</ul>\n<p>These are often provided with tutorials of<span>&nbsp;</span><a title="Constraint programming" href="https://en.wikipedia.org/wiki/Constraint_programming">CP</a>, ASP, Boolean SAT and SMT solvers. In the general case, constraint problems can be much harder, and may not be expressible in some of these simpler systems. "Real life" examples include<span>&nbsp;</span><a class="mw-redirect" title="Automated planning" href="https://en.wikipedia.org/wiki/Automated_planning">automated planning</a>,<sup id="cite_ref-GhallabNau2004_6-0" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-GhallabNau2004-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-7" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a class="mw-redirect" title="Lexical disambiguation" href="https://en.wikipedia.org/wiki/Lexical_disambiguation">lexical disambiguation</a>,<sup id="cite_ref-8" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-8"><span class="cite-bracket">[</span>8<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-9" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-9"><span class="cite-bracket">[</span>9<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Musicology" href="https://en.wikipedia.org/wiki/Musicology">musicology</a>,<sup id="cite_ref-10" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-10"><span class="cite-bracket">[</span>10<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Configure, price and quote" href="https://en.wikipedia.org/wiki/Configure,_price_and_quote">product configuration</a><sup id="cite_ref-11" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-11"><span class="cite-bracket">[</span>11<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>and<span>&nbsp;</span><a title="Resource allocation" href="https://en.wikipedia.org/wiki/Resource_allocation">resource allocation</a>.<sup id="cite_ref-12" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup></p>\n<p>The existence of a solution to a CSP can be viewed as a<span>&nbsp;</span><a title="Decision problem" href="https://en.wikipedia.org/wiki/Decision_problem">decision problem</a>. This can be decided by finding a solution, or failing to find a solution after exhaustive search (<a class="mw-redirect" title="Stochastic algorithm" href="https://en.wikipedia.org/wiki/Stochastic_algorithm">stochastic algorithms</a><span>&nbsp;</span>typically never reach an exhaustive conclusion, while directed searches often do, on sufficiently small problems). In some cases the CSP might be known to have solutions beforehand, through some other mathematical inference process.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Formal_definition">Formal definition</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Formal definition" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=1"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Formally, a constraint satisfaction problem is defined as a triple<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\langle X,D,C\\rangle }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo fence="false" stretchy="false">⟨</mo><mi>X</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>C</mi><mo fence="false" stretchy="false">⟩</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f2e74db7a51e7e2bd4993c61fc60fb5d2f23ef6f" alt="{\\displaystyle \\langle X,D,C\\rangle }" aria-hidden="true" loading="lazy"></span>, where<sup id="cite_ref-Russell2010_13-0" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-Russell2010-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup></p>\n<ul>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle X=\\{X_{1},\\ldots ,X_{n}\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>X</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mi>X</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>X</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></msub><mo fence="false" stretchy="false">}</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dcbf39e54811783de5bfd141d419c65a4845dde2" alt="{\\displaystyle X=\\{X_{1},\\ldots ,X_{n}\\}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a set of variables,</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle D=\\{D_{1},\\ldots ,D_{n}\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>D</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow></msub><mo fence="false" stretchy="false">}</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/169fc4f57811304de0d1efffd1e4dba3152eba91" alt="{\\displaystyle D=\\{D_{1},\\ldots ,D_{n}\\}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a set of their respective domains of values, and</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle C=\\{C_{1},\\ldots ,C_{m}\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>C</mi><mo>=</mo><mo fence="false" stretchy="false">{</mo><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mi>m</mi></mrow></msub><mo fence="false" stretchy="false">}</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d49c92e67face9fb811ad2f10c6bb1ce1804a6cd" alt="{\\displaystyle C=\\{C_{1},\\ldots ,C_{m}\\}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a set of constraints.</li>\n</ul>\n<p>Each variable<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle X_{i}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>X</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af4a0955af42beb5f85aa05fb8c07abedc13990d" alt="{\\displaystyle X_{i}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>can take on the values in the nonempty domain<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle D_{i}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9f07b53d3212e08ca316a536c8aac0bbefa79ee1" alt="{\\displaystyle D_{i}}" aria-hidden="true" loading="lazy"></span>. Every constraint<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle C_{j}\\in C}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>C</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>∈</mo><mi>C</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fb599c15e70abc26200f17997299d3e935befaa" alt="{\\displaystyle C_{j}\\in C}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is in turn a pair<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\langle t_{j},R_{j}\\rangle }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo fence="false" stretchy="false">⟨</mo><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo fence="false" stretchy="false">⟩</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/668bed52faba4d03b23bb64891c65ce024da7017" alt="{\\displaystyle \\langle t_{j},R_{j}\\rangle }" aria-hidden="true" loading="lazy"></span>, where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle t_{j}\\subseteq \\{1,2,\\ldots ,n\\}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>⊆</mo><mo fence="false" stretchy="false">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>n</mi><mo fence="false" stretchy="false">}</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3061f944959af2bd28b9efac4d621723398faac3" alt="{\\displaystyle t_{j}\\subseteq \\{1,2,\\ldots ,n\\}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a set of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle k}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>k</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" alt="{\\displaystyle k}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>indices and<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{j}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bc93d61e1436bb2c2fb771a13d0892784754998" alt="{\\displaystyle R_{j}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle k}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>k</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40" alt="{\\displaystyle k}" aria-hidden="true" loading="lazy"></span>-ary<span>&nbsp;</span><a title="Relation (mathematics)" href="https://en.wikipedia.org/wiki/Relation_(mathematics)">relation</a><span>&nbsp;</span>on the corresponding product of domains<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\times _{i\\in t_{j}}D_{i}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mo>×</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>∈</mo><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mrow></msub><msub><mi>D</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f24422ad0e18a56db579bdaae8aca1b95bff71fd" alt="{\\displaystyle \\times _{i\\in t_{j}}D_{i}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>where the product is taken with indices in ascending order. An<span>&nbsp;</span><i>evaluation</i><span>&nbsp;</span>of the variables is a function from a subset of variables to a particular set of values in the corresponding subset of domains. An evaluation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle v}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>v</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e07b00e7fc0847fbd16391c778d65bc25c452597" alt="{\\displaystyle v}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>satisfies a constraint<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\langle t_{j},R_{j}\\rangle }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo fence="false" stretchy="false">⟨</mo><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub><mo fence="false" stretchy="false">⟩</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/668bed52faba4d03b23bb64891c65ce024da7017" alt="{\\displaystyle \\langle t_{j},R_{j}\\rangle }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>if the values assigned to the variables<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle t_{j}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53942a7888623b7eff84a0e43183e046c9f66d65" alt="{\\displaystyle t_{j}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>satisfy the relation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{j}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>j</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bc93d61e1436bb2c2fb771a13d0892784754998" alt="{\\displaystyle R_{j}}" aria-hidden="true" loading="lazy"></span>.</p>\n<p>An evaluation is<span>&nbsp;</span><i>consistent</i><span>&nbsp;</span>if it does not violate any of the constraints. An evaluation is<span>&nbsp;</span><i>complete</i><span>&nbsp;</span>if it includes all variables. An evaluation is a<span>&nbsp;</span><i>solution</i><span>&nbsp;</span>if it is consistent and complete; such an evaluation is said to<span>&nbsp;</span><i>solve</i><span>&nbsp;</span>the constraint satisfaction problem.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Solution">Solution</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Solution" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=2"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Constraint satisfaction problems on finite domains are typically solved using a form of<span>&nbsp;</span><a title="Search algorithm" href="https://en.wikipedia.org/wiki/Search_algorithm">search</a>. The most used techniques are variants of<span>&nbsp;</span><a title="Backtracking" href="https://en.wikipedia.org/wiki/Backtracking">backtracking</a>,<span>&nbsp;</span><a class="mw-redirect" title="Constraint propagation" href="https://en.wikipedia.org/wiki/Constraint_propagation">constraint propagation</a>, and<span>&nbsp;</span><a title="Local search (optimization)" href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local search</a>. These techniques are also often combined, as in the<span>&nbsp;</span><a title="Very large-scale neighborhood search" href="https://en.wikipedia.org/wiki/Very_large-scale_neighborhood_search">VLNS</a><span>&nbsp;</span>method, and current research involves other technologies such as<span>&nbsp;</span><a title="Linear programming" href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a>.<sup id="cite_ref-14" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup></p>\n<p><a title="Backtracking" href="https://en.wikipedia.org/wiki/Backtracking">Backtracking</a><span>&nbsp;</span>is a recursive algorithm. It maintains a partial assignment of the variables. Initially, all variables are unassigned. At each step, a variable is chosen, and all possible values are assigned to it in turn. For each value, the consistency of the partial assignment with the constraints is checked; in case of consistency, a<span>&nbsp;</span><a title="Recursion" href="https://en.wikipedia.org/wiki/Recursion">recursive</a><span>&nbsp;</span>call is performed. When all values have been tried, the algorithm backtracks. In this basic backtracking algorithm, consistency is defined as the satisfaction of all constraints whose variables are all assigned. Several variants of backtracking exist.<span>&nbsp;</span><a title="Backmarking" href="https://en.wikipedia.org/wiki/Backmarking">Backmarking</a><span>&nbsp;</span>improves the efficiency of checking consistency.<span>&nbsp;</span><a title="Backjumping" href="https://en.wikipedia.org/wiki/Backjumping">Backjumping</a><span>&nbsp;</span>allows saving part of the search by backtracking "more than one variable" in some cases.<span>&nbsp;</span><a title="Constraint learning" href="https://en.wikipedia.org/wiki/Constraint_learning">Constraint learning</a><span>&nbsp;</span>infers and saves new constraints that can be later used to avoid part of the search.<span>&nbsp;</span><a title="Look-ahead (backtracking)" href="https://en.wikipedia.org/wiki/Look-ahead_(backtracking)">Look-ahead</a><span>&nbsp;</span>is also often used in backtracking to attempt to foresee the effects of choosing a variable or a value, thus sometimes determining in advance when a subproblem is satisfiable or unsatisfiable.</p>\n<p><a class="mw-redirect" title="Constraint propagation" href="https://en.wikipedia.org/wiki/Constraint_propagation">Constraint propagation</a><span>&nbsp;</span>techniques are methods used to modify a constraint satisfaction problem. More precisely, they are methods that enforce a form of<span>&nbsp;</span><a title="Local consistency" href="https://en.wikipedia.org/wiki/Local_consistency">local consistency</a>, which are conditions related to the consistency of a group of variables and/or constraints. Constraint propagation has various uses. First, it turns a problem into one that is equivalent but is usually simpler to solve. Second, it may prove satisfiability or unsatisfiability of problems. This is not guaranteed to happen in general; however, it always happens for some forms of constraint propagation and/or for certain kinds of problems. The most known and used forms of local consistency are<span>&nbsp;</span><a class="mw-redirect" title="Arc consistency" href="https://en.wikipedia.org/wiki/Arc_consistency">arc consistency</a>,<span>&nbsp;</span><a class="mw-redirect" title="Hyper-arc consistency" href="https://en.wikipedia.org/wiki/Hyper-arc_consistency">hyper-arc consistency</a>, and<span>&nbsp;</span><a class="mw-redirect" title="Path consistency" href="https://en.wikipedia.org/wiki/Path_consistency">path consistency</a>. The most popular constraint propagation method is the<span>&nbsp;</span><a title="AC-3 algorithm" href="https://en.wikipedia.org/wiki/AC-3_algorithm">AC-3 algorithm</a>, which enforces arc consistency.</p>\n<p><a title="Local search (optimization)" href="https://en.wikipedia.org/wiki/Local_search_(optimization)">Local search</a><span>&nbsp;</span>methods are incomplete satisfiability algorithms. They may find a solution of a problem, but they may fail even if the problem is satisfiable. They work by iteratively improving a complete assignment over the variables. At each step, a small number of variables are changed in value, with the overall aim of increasing the number of constraints satisfied by this assignment. The<span>&nbsp;</span><a title="Min-conflicts algorithm" href="https://en.wikipedia.org/wiki/Min-conflicts_algorithm">min-conflicts algorithm</a><span>&nbsp;</span>is a local search algorithm specific for CSPs and is based on that principle. In practice, local search appears to work well when these changes are also affected by random choices. An integration of search with local search has been developed, leading to<span>&nbsp;</span><a title="Hybrid algorithm (constraint satisfaction)" href="https://en.wikipedia.org/wiki/Hybrid_algorithm_(constraint_satisfaction)">hybrid algorithms</a>.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Theoretical_aspects">Theoretical aspects</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Theoretical aspects" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=3"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="mw-heading mw-heading3">\n<h3 id="Computational_Complexity">Computational Complexity</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Computational Complexity" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=4"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>CSPs are also studied in<span>&nbsp;</span><a title="Computational complexity theory" href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity theory</a>,<span>&nbsp;</span><a title="Finite model theory" href="https://en.wikipedia.org/wiki/Finite_model_theory">finite model theory</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Universal algebra" href="https://en.wikipedia.org/wiki/Universal_algebra">universal algebra</a>. It turned out that questions about the complexity of CSPs translate into important universal-algebraic questions about underlying algebras. This approach is known as the<span>&nbsp;</span><i>algebraic approach</i><span>&nbsp;</span>to CSPs.<sup id="cite_ref-15" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-15"><span class="cite-bracket">[</span>15<span class="cite-bracket">]</span></a></sup></p>\n<p>Since every computational decision problem is<span>&nbsp;</span><a title="Polynomial-time reduction" href="https://en.wikipedia.org/wiki/Polynomial-time_reduction">polynomial-time equivalent</a><span>&nbsp;</span>to a CSP with an infinite template,<sup id="cite_ref-16" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-16"><span class="cite-bracket">[</span>16<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>general CSPs can have arbitrary complexity. In particular, there are also CSPs within the class of<span>&nbsp;</span><a title="NP-intermediate" href="https://en.wikipedia.org/wiki/NP-intermediate">NP-intermediate</a><span>&nbsp;</span>problems, whose existence was demonstrated by<span>&nbsp;</span><a title="NP-intermediate" href="https://en.wikipedia.org/wiki/NP-intermediate">Ladner</a>, under the assumption that<span>&nbsp;</span><a title="P versus NP problem" href="https://en.wikipedia.org/wiki/P_versus_NP_problem">P ≠ NP</a>.</p>\n<p>However, a large class of CSPs arising from natural applications satisfy a complexity dichotomy, meaning that every CSP within that class is either in<span>&nbsp;</span><a title="P (complexity)" href="https://en.wikipedia.org/wiki/P_(complexity)">P</a><span>&nbsp;</span>or<span>&nbsp;</span><a class="mw-redirect" title="NP-complete" href="https://en.wikipedia.org/wiki/NP-complete">NP-complete</a>. These CSPs thus provide one of the largest known subsets of<span>&nbsp;</span><a title="NP (complexity)" href="https://en.wikipedia.org/wiki/NP_(complexity)">NP</a><span>&nbsp;</span>which avoids<span>&nbsp;</span><a title="NP-intermediate" href="https://en.wikipedia.org/wiki/NP-intermediate">NP-intermediate</a><span>&nbsp;</span>problems. A complexity dichotomy was first proven by<span>&nbsp;</span><a title="Schaefer\'s dichotomy theorem" href="https://en.wikipedia.org/wiki/Schaefer%27s_dichotomy_theorem">Schaefer</a><span>&nbsp;</span>for Boolean CSPs, i.e. CSPs over a 2-element domain and where all the available relations are<span>&nbsp;</span><a class="mw-redirect" title="Boolean operator (Boolean algebra)" href="https://en.wikipedia.org/wiki/Boolean_operator_(Boolean_algebra)">Boolean operators</a>. This result has been generalized for various classes of CSPs, most notably for all CSPs over finite domains. This<span>&nbsp;</span><i>finite-domain dichotomy conjecture</i><span>&nbsp;</span>was first formulated by Tomás Feder and Moshe Vardi,<sup id="cite_ref-17" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-17"><span class="cite-bracket">[</span>17<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>and finally proven independently by Andrei Bulatov<sup id="cite_ref-18" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-18"><span class="cite-bracket">[</span>18<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>and Dmitriy Zhuk in 2017.<sup id="cite_ref-19" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-19"><span class="cite-bracket">[</span>19<span class="cite-bracket">]</span></a></sup></p>\n<p>Other classes for which a complexity dichotomy has been confirmed are</p>\n<ul>\n<li>all<span>&nbsp;</span><a title="First-order logic" href="https://en.wikipedia.org/wiki/First-order_logic">first-order</a><span>&nbsp;</span><a title="Reduct" href="https://en.wikipedia.org/wiki/Reduct">reducts</a><span>&nbsp;</span>of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (\\mathbb {Q} ,<)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">Q</mi></mrow><mo>,</mo><mo>&lt;</mo><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/43c487a052f5cefd2a85361e480e641af11aabb3" alt="{\\displaystyle (\\mathbb {Q} ,<)}" aria-hidden="true" loading="lazy"></span>,<sup id="cite_ref-20" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-20"><span class="cite-bracket">[</span>20<span class="cite-bracket">]</span></a></sup></li>\n<li>all first-order reducts of the<span>&nbsp;</span><a title="Rado graph" href="https://en.wikipedia.org/wiki/Rado_graph">countable random graph</a>,<sup id="cite_ref-21" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup></li>\n<li>all first-order reducts of the<span>&nbsp;</span><a class="mw-redirect" title="Model companion" href="https://en.wikipedia.org/wiki/Model_companion">model companion</a><span>&nbsp;</span>of the class of all C-relations,<sup id="cite_ref-22" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-22"><span class="cite-bracket">[</span>22<span class="cite-bracket">]</span></a></sup></li>\n<li>all first-order reducts of the universal homogenous<span>&nbsp;</span><a title="Partially ordered set" href="https://en.wikipedia.org/wiki/Partially_ordered_set">poset</a>,<sup id="cite_ref-23" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-23"><span class="cite-bracket">[</span>23<span class="cite-bracket">]</span></a></sup></li>\n<li>all first-order reducts of homogenous undirected graphs,<sup id="cite_ref-24" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-24"><span class="cite-bracket">[</span>24<span class="cite-bracket">]</span></a></sup></li>\n<li>all first-order reducts of all unary structures,<sup id="cite_ref-25" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-25"><span class="cite-bracket">[</span>25<span class="cite-bracket">]</span></a></sup></li>\n<li>all CSPs in the complexity class MMSNP.<sup id="cite_ref-26" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-26"><span class="cite-bracket">[</span>26<span class="cite-bracket">]</span></a></sup></li>\n</ul>\n<p>Most classes of CSPs that are known to be tractable are those where the<span>&nbsp;</span><a title="Hypergraph" href="https://en.wikipedia.org/wiki/Hypergraph">hypergraph</a><span>&nbsp;</span>of constraints has bounded<span>&nbsp;</span><a title="Treewidth" href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>,<sup id="cite_ref-27" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-27"><span class="cite-bracket">[</span>27<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>or where the constraints have arbitrary form but there exist equationally non-trivial polymorphisms of the set of constraint relations.<sup id="cite_ref-28" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-28"><span class="cite-bracket">[</span>28<span class="cite-bracket">]</span></a></sup></p>\n<p>An<span>&nbsp;</span><i>infinite-domain dichotomy conjecture</i><sup id="cite_ref-29" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-29"><span class="cite-bracket">[</span>29<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>has been formulated for all CSPs of reducts of finitely bounded homogenous structures, stating that the CSP of such a structure is in P if and only if its<span>&nbsp;</span><a title="Clone (algebra)" href="https://en.wikipedia.org/wiki/Clone_(algebra)">polymorphism clone</a><span>&nbsp;</span>is equationally non-trivial, and NP-hard otherwise.</p>\n<p>The complexity of such infinite-domain CSPs as well as of other generalisations (Valued CSPs, Quantified CSPs, Promise CSPs) is still an area of active research.<sup id="cite_ref-30" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-30"><span class="cite-bracket">[</span>30<span class="cite-bracket">]</span></a></sup><a class="external autonumber" href="https://tu-dresden.de/tu-dresden/newsportal/news/erc-synergy-grant-fuer-pococop-komplexitaet-von-berechnungen?set_language=en">[1]</a><a class="external autonumber" href="https://www.tuwien.at/tu-wien/aktuelles/news/erc-synergy-grant-die-komplexitaet-von-berechnungen">[2]</a></p>\n<p>Every CSP can also be considered as a<span>&nbsp;</span><a title="Conjunctive query" href="https://en.wikipedia.org/wiki/Conjunctive_query">conjunctive query</a><span>&nbsp;</span>containment problem.<sup id="cite_ref-31" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-31"><span class="cite-bracket">[</span>31<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Function_problems">Function problems</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Function problems" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=5"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>A similar situation exists between the functional classes<span>&nbsp;</span><a title="FP (complexity)" href="https://en.wikipedia.org/wiki/FP_(complexity)">FP</a><span>&nbsp;</span>and<span>&nbsp;</span><a class="mw-redirect" title="Sharp-P" href="https://en.wikipedia.org/wiki/Sharp-P">#P</a>. By a generalization of<span>&nbsp;</span><a class="mw-redirect" title="Ladner\'s theorem" href="https://en.wikipedia.org/wiki/Ladner%27s_theorem">Ladner\'s theorem</a>, there are also problems in neither FP nor<span>&nbsp;</span><a class="mw-redirect" title="Sharp-P-complete" href="https://en.wikipedia.org/wiki/Sharp-P-complete">#P-complete</a><span>&nbsp;</span>as long as FP ≠ #P. As in the decision case, a problem in the #CSP is defined by a set of relations. Each problem takes a<span>&nbsp;</span><a class="mw-redirect" title="Boolean logic" href="https://en.wikipedia.org/wiki/Boolean_logic">Boolean</a><span>&nbsp;</span>formula as input and the task is to compute the number of satisfying assignments. This can be further generalized by using larger domain sizes and attaching a weight to each satisfying assignment and computing the sum of these weights. It is known that any complex weighted #CSP problem is either in FP or #P-hard.<sup id="cite_ref-32" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-32"><span class="cite-bracket">[</span>32<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Variants">Variants</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Variants" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=6"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The classic model of Constraint Satisfaction Problem defines a model of static, inflexible constraints. This rigid model is a shortcoming that makes it difficult to represent problems easily.<sup id="cite_ref-33" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-33"><span class="cite-bracket">[</span>33<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Several modifications of the basic CSP definition have been proposed to adapt the model to a wide variety of problems.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Dynamic_CSPs">Dynamic CSPs</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Dynamic CSPs" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=7"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p><strong>Dynamic CSPs</strong><sup id="cite_ref-34" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-34"><span class="cite-bracket">[</span>34<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>(<i>DCSP</i>s) are useful when the original formulation of a problem is altered in some way, typically because the set of constraints to consider evolves because of the environment.<sup id="cite_ref-35" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-35"><span class="cite-bracket">[</span>35<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>DCSPs are viewed as a sequence of static CSPs, each one a transformation of the previous one in which variables and constraints can be added (restriction) or removed (relaxation). Information found in the initial formulations of the problem can be used to refine the next ones. The solving method can be classified according to the way in which information is transferred:</p>\n<ul>\n<li><a title="Oracle machine" href="https://en.wikipedia.org/wiki/Oracle_machine">Oracles</a>: the solution found to previous CSPs in the sequence are used as heuristics to guide the resolution of the current CSP from scratch.</li>\n<li>Local repair: each CSP is calculated starting from the partial solution of the previous one and repairing the inconsistent constraints with<span>&nbsp;</span><a title="Local search (optimization)" href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local search</a>.</li>\n<li>Constraint recording: new constraints are defined in each stage of the search to represent the learning of inconsistent group of decisions. Those constraints are carried over to the new CSP problems.</li>\n</ul>\n<div class="mw-heading mw-heading3">\n<h3 id="Flexible_CSPs">Flexible CSPs</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Flexible CSPs" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=8"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Classic CSPs treat constraints as hard, meaning that they are<span>&nbsp;</span><i>imperative</i><span>&nbsp;</span>(each solution must satisfy all of them) and<span>&nbsp;</span><i>inflexible</i><span>&nbsp;</span>(in the sense that they must be completely satisfied or else they are completely violated).<span>&nbsp;</span><strong>Flexible CSP</strong>s relax those assumptions, partially<span>&nbsp;</span><i>relaxing</i><span>&nbsp;</span>the constraints and allowing the solution to not comply with all of them. This is similar to preferences in<span>&nbsp;</span><a title="Preference-based planning" href="https://en.wikipedia.org/wiki/Preference-based_planning">preference-based planning</a>. Some types of flexible CSPs include:</p>\n<ul>\n<li>MAX-CSP, where a number of constraints are allowed to be violated, and the quality of a solution is measured by the number of satisfied constraints.</li>\n<li><a title="Weighted constraint satisfaction problem" href="https://en.wikipedia.org/wiki/Weighted_constraint_satisfaction_problem">Weighted CSP</a>, a MAX-CSP in which each violation of a constraint is weighted according to a predefined preference. Thus satisfying constraint with more weight is preferred.</li>\n<li>Fuzzy CSP model constraints as<span>&nbsp;</span><a title="Fuzzy logic" href="https://en.wikipedia.org/wiki/Fuzzy_logic">fuzzy</a><span>&nbsp;</span>relations in which the satisfaction of a constraint is a continuous function of its variables\' values, going from fully satisfied to fully violated.</li>\n</ul>\n<div class="mw-heading mw-heading3">\n<h3 id="Decentralized_CSPs">Decentralized CSPs</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Decentralized CSPs" href="https://en.wikipedia.org/w/index.php?title=Constraint_satisfaction_problem&amp;action=edit&amp;section=9"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In DCSPs<sup id="cite_ref-cfl_36-0" class="reference"><a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem#cite_note-cfl-36"><span class="cite-bracket">[</span>36<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>each constraint variable is thought of as having a separate geographic location. Strong constraints are placed on information exchange between variables, requiring the use of fully distributed algorithms to solve the constraint satisfaction problem.</p>\n</div>\n</div>\n</div>\n</div>\n<div class="mw-footer-container"><footer id="footer" class="mw-footer">\n<p>&nbsp;</p>\n</footer></div>\n</div>\n</div><script src="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3521935/dp_app.js"></script>'

reinforcement_learning_body = '<link rel="stylesheet" href="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3563198/dp_colors_uit_blue.css"><div class="mw-page-container">\n<div class="mw-page-container-inner">\n<div class="mw-content-container">\n<div id="bodyContent" class="vector-body ve-init-mw-desktopArticleTarget-targetContainer" aria-labelledby="firstHeading" data-mw-ve-target-container="">\n<div id="mw-content-text" class="mw-body-content">\n<div class="mw-content-ltr mw-parser-output" dir="ltr" lang="en">\n<p><strong>Reinforcement learning</strong><span>&nbsp;</span>(<strong>RL</strong>) is an interdisciplinary area of<span>&nbsp;</span><a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Optimal control" href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a><span>&nbsp;</span>concerned with how an<span>&nbsp;</span><a title="Intelligent agent" href="https://en.wikipedia.org/wiki/Intelligent_agent">intelligent agent</a><span>&nbsp;</span>should<span>&nbsp;</span><a title="Action selection" href="https://en.wikipedia.org/wiki/Action_selection">take actions</a><span>&nbsp;</span>in a dynamic environment in order to<span>&nbsp;</span><a title="Reward-based selection" href="https://en.wikipedia.org/wiki/Reward-based_selection">maximize a reward</a><span>&nbsp;</span>signal. Reinforcement learning is one of the<span>&nbsp;</span><a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning#Approaches">three basic machine learning paradigms</a>, alongside<span>&nbsp;</span><a title="Supervised learning" href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Unsupervised learning" href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a>.</p>\n<p>Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).<sup id="cite_ref-kaelbling_1-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-kaelbling-1"><span class="cite-bracket">[</span>1<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>The search for this balance is known as the<span>&nbsp;</span><a title="Exploration–exploitation dilemma" href="https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma">exploration–exploitation dilemma</a>.</p>\n<p>The environment is typically stated in the form of a<span>&nbsp;</span><a title="Markov decision process" href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a><span>&nbsp;</span>(MDP), as many reinforcement learning algorithms use<span>&nbsp;</span><a title="Dynamic programming" href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a><span>&nbsp;</span>techniques.<sup id="cite_ref-2" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-2"><span class="cite-bracket">[</span>2<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.<sup id="cite_ref-Li-2023_3-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-Li-2023-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Principles">Principles</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Principles" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=1"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Due to its generality, reinforcement learning is studied in many disciplines, such as<span>&nbsp;</span><a title="Game theory" href="https://en.wikipedia.org/wiki/Game_theory">game theory</a>,<span>&nbsp;</span><a title="Control theory" href="https://en.wikipedia.org/wiki/Control_theory">control theory</a>,<span>&nbsp;</span><a title="Operations research" href="https://en.wikipedia.org/wiki/Operations_research">operations research</a>,<span>&nbsp;</span><a title="Information theory" href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>,<span>&nbsp;</span><a title="Simulation-based optimization" href="https://en.wikipedia.org/wiki/Simulation-based_optimization">simulation-based optimization</a>,<span>&nbsp;</span><a title="Multi-agent system" href="https://en.wikipedia.org/wiki/Multi-agent_system">multi-agent systems</a>,<span>&nbsp;</span><a title="Swarm intelligence" href="https://en.wikipedia.org/wiki/Swarm_intelligence">swarm intelligence</a>, and<span>&nbsp;</span><a title="Statistics" href="https://en.wikipedia.org/wiki/Statistics">statistics</a>. In the operations research and control literature, RL is called<span>&nbsp;</span><i>approximate dynamic programming</i>, or<span>&nbsp;</span><i>neuro-dynamic programming.</i><span>&nbsp;</span>The problems of interest in RL have also been studied in the<span>&nbsp;</span><a class="mw-redirect" title="Optimal control theory" href="https://en.wikipedia.org/wiki/Optimal_control_theory">theory of optimal control</a>, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).</p>\n<p>Basic reinforcement learning is modeled as a<span>&nbsp;</span><a title="Markov decision process" href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a>:</p>\n<ul>\n<li>A set of environment and agent states (the state space),<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\mathcal {S}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">S</mi></mrow></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2302a18e269dbecc43c57c0c2aced3bfae15278d" alt="{\\displaystyle {\\mathcal {S}}}" aria-hidden="true" loading="lazy"></span>;</li>\n<li>A set of actions (the action space),<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle {\\mathcal {A}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/280ae03440942ab348c2ca9b8db6b56ffa9618f8" alt="{\\displaystyle {\\mathcal {A}}}" aria-hidden="true" loading="lazy"></span>, of the agent;</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle P_{a}(s,s\')=\\Pr(S_{t+1}=s\'\\mid S_{t}=s,A_{t}=a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>P</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msup><mi>s</mi><mo>′</mo></msup><mo>∣</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e76fa3fccba8ec1365ed78c382313331381fb625" alt="{\\displaystyle P_{a}(s,s\')=\\Pr(S_{t+1}=s\'\\mid S_{t}=s,A_{t}=a)}" aria-hidden="true" loading="lazy"></span>, the transition probability (at time<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle t}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>t</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65658b7b223af9e1acc877d848888ecdb4466560" alt="{\\displaystyle t}" aria-hidden="true" loading="lazy"></span>) from state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>under action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span>.</li>\n<li><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f842d4a4b1340e194d8014d63163e5e27f94215" alt="{\\displaystyle R_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span>, the immediate reward after transition from<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>under action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span>.</li>\n</ul>\n<p>The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to<span>&nbsp;</span><a title="Reinforcement" href="https://en.wikipedia.org/wiki/Reinforcement">processes</a><span>&nbsp;</span>that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.<sup id="cite_ref-4" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-4"><span class="cite-bracket">[</span>4<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-5" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-5"><span class="cite-bracket">[</span>5<span class="cite-bracket">]</span></a></sup></p>\n<p>A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step<span>&nbsp;</span><span class="texhtml mvar">t</span>, the agent receives the current state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S_{t}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e2391e6e796fbf718be3828080775ac2ac3d3d4" alt="{\\displaystyle S_{t}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and reward<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{t}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d65b678ee539ac36de96b554af181ac03b7f16a8" alt="{\\displaystyle R_{t}}" aria-hidden="true" loading="lazy"></span>. It then chooses an action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle A_{t}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/265483c517cb98cde609f03a31964d86cdcb05c9" alt="{\\displaystyle A_{t}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S_{t+1}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7fde0b9d29bcb7e8d3eee3a8f42ead14aa3b8cee" alt="{\\displaystyle S_{t+1}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and the reward<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{t+1}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/78bc3f077c19f7db4e89b5529a5861b06ae782b7" alt="{\\displaystyle R_{t+1}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>associated with the<span>&nbsp;</span><i>transition</i><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (S_{t},A_{t},S_{t+1})}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efa800ac9fbf81e68eb97515e04cf99ef35b730d" alt="{\\displaystyle (S_{t},A_{t},S_{t+1})}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is determined. The goal of a reinforcement learning agent is to learn a<span>&nbsp;</span><i>policy</i>:</p>\n<p><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo>:</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">S</mi></mrow></mrow><mo>×</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></mrow><mo stretchy="false">→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a69266603ad25d7978aa2bb47fddf8a28aa3399" alt="{\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}" aria-hidden="true" loading="lazy"></span>,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>a</mi><mo>∣</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0e6b2d563f62175e044ff901314c4557840ca9d" alt="{\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}" aria-hidden="true" loading="lazy"></span></p>\n<p>that maximizes the expected cumulative reward.</p>\n<p>Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have<span>&nbsp;</span><i>full observability</i>. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have<span>&nbsp;</span><i>partial observability</i>, and formally the problem must be formulated as a<span>&nbsp;</span><a title="Partially observable Markov decision process" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">partially observable Markov decision process</a>. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.</p>\n<p>When the agent\'s performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of<span>&nbsp;</span><a title="Regret (decision theory)" href="https://en.wikipedia.org/wiki/Regret_(decision_theory)">regret</a>. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.</p>\n<p>Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including<span>&nbsp;</span><a title="Energy storage" href="https://en.wikipedia.org/wiki/Energy_storage">energy storage</a>,<sup id="cite_ref-6" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-6"><span class="cite-bracket">[</span>6<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Robot control" href="https://en.wikipedia.org/wiki/Robot_control">robot control</a>,<sup id="cite_ref-7" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-7"><span class="cite-bracket">[</span>7<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Photovoltaic system" href="https://en.wikipedia.org/wiki/Photovoltaic_system">photovoltaic generators</a>,<sup id="cite_ref-8" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-8"><span class="cite-bracket">[</span>8<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Backgammon" href="https://en.wikipedia.org/wiki/Backgammon">backgammon</a>,<span>&nbsp;</span><a title="Checkers" href="https://en.wikipedia.org/wiki/Checkers">checkers</a>,<sup id="cite_ref-FOOTNOTESuttonBarto2018Chapter_11_9-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-FOOTNOTESuttonBarto2018Chapter_11-9"><span class="cite-bracket">[</span>9<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span><a title="Go (game)" href="https://en.wikipedia.org/wiki/Go_(game)">Go</a><span>&nbsp;</span>(<a title="AlphaGo" href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>), and<span>&nbsp;</span><a title="Self-driving car" href="https://en.wikipedia.org/wiki/Self-driving_car">autonomous driving systems</a>.<sup id="cite_ref-Ren-2022_10-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-Ren-2022-10"><span class="cite-bracket">[</span>10<span class="cite-bracket">]</span></a></sup></p>\n<p>Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of<span>&nbsp;</span><a title="Neural network (machine learning)" href="https://en.wikipedia.org/wiki/Neural_network_(machine_learning)">function approximation</a><span>&nbsp;</span>to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:</p>\n<ul>\n<li>A model of the environment is known, but an<span>&nbsp;</span><a title="Closed-form expression" href="https://en.wikipedia.org/wiki/Closed-form_expression">analytic solution</a><span>&nbsp;</span>is not available;</li>\n<li>Only a simulation model of the environment is given (the subject of<span>&nbsp;</span><a title="Simulation-based optimization" href="https://en.wikipedia.org/wiki/Simulation-based_optimization">simulation-based optimization</a>);<sup id="cite_ref-11" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-11"><span class="cite-bracket">[</span>11<span class="cite-bracket">]</span></a></sup></li>\n<li>The only way to collect information about the environment is to interact with it.</li>\n</ul>\n<p>The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to<span>&nbsp;</span><a title="Machine learning" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a><span>&nbsp;</span>problems.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Exploration">Exploration</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Exploration" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=2"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The exploration vs. exploitation trade-off has been most thoroughly studied through the<span>&nbsp;</span><a title="Multi-armed bandit" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandit</a><span>&nbsp;</span>problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).<sup id="cite_ref-Optimal_adaptive_policies_for_Marko_12-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-Optimal_adaptive_policies_for_Marko-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup></p>\n<p>Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.</p>\n<p>One such method is<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\varepsilon }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ε</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" alt="{\\displaystyle \\varepsilon }" aria-hidden="true" loading="lazy"></span>-greedy, where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 0<\\varepsilon <1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn><mo>&lt;</mo><mi>ε</mi><mo>&lt;</mo><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a62c0bbc36352f492bde6217430cfe342032487d" alt="{\\displaystyle 0<\\varepsilon <1}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a parameter controlling the amount of exploration vs. exploitation. With probability<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 1-\\varepsilon }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>1</mn><mo>−</mo><mi>ε</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/09646b74d0bca9befab4b7e9456a44a20d250ace" alt="{\\displaystyle 1-\\varepsilon }" aria-hidden="true" loading="lazy"></span>, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\varepsilon }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ε</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" alt="{\\displaystyle \\varepsilon }" aria-hidden="true" loading="lazy"></span>, exploration is chosen, and the action is chosen uniformly at random.<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\varepsilon }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ε</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a30c89172e5b88edbd45d3e2772c7f5e562e5173" alt="{\\displaystyle \\varepsilon }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.<sup id="cite_ref-13" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-13"><span class="cite-bracket">[</span>13<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Algorithms_for_control_learning">Algorithms for control learning</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Algorithms for control learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=3"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Criterion_of_optimality">Criterion of optimality</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Criterion of optimality" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=4"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="mw-heading mw-heading4">\n<h4 id="Policy">Policy</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Policy" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=5"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The agent\'s action selection is modeled as a map called<span>&nbsp;</span><i>policy</i>:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi :{\\mathcal {A}}\\times {\\mathcal {S}}\\rightarrow [0,1]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo>:</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">A</mi></mrow></mrow><mo>×</mo><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">S</mi></mrow></mrow><mo stretchy="false">→</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8874b7ef5cb4db70c52a318d73a094611a33f5f4" alt="{\\displaystyle \\pi :{\\mathcal {A}}\\times {\\mathcal {S}}\\rightarrow [0,1]}" aria-hidden="true" loading="lazy"></span></dd>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (a,s)=\\Pr(A_{t}=a\\mid S_{t}=s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>A</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>a</mi><mo>∣</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mo>=</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae6511085767b3088048f2a78a5e581bda4b7349" alt="{\\displaystyle \\pi (a,s)=\\Pr(A_{t}=a\\mid S_{t}=s)}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>The policy map gives the probability of taking action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>when in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>.<sup id="cite_ref-:0_14-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-:0-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup><sup class="reference nowrap"><span title="Page / location: 61">: 61 </span></sup><span>&nbsp;</span>There are also deterministic policies<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>for which<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi (s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3df25793e1e7a06faa8e939a794c8de8b4459bf" alt="{\\displaystyle \\pi (s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>denotes the action that should be played at state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="State-value_function">State-value function</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: State-value function" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=6"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The state-value function<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V_{\\pi }(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04f08b168d05891950e565017e284abee8bf7cf1" alt="{\\displaystyle V_{\\pi }(s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is defined as,<span>&nbsp;</span><i>expected discounted return</i><span>&nbsp;</span>starting with state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>, i.e.<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S_{0}=s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub><mo>=</mo><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/76b9df1ecbdbd4dbae77931a7aa185959d3b55f3" alt="{\\displaystyle S_{0}=s}" aria-hidden="true" loading="lazy"></span>, and successively following policy<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span>. Hence, roughly speaking, the value function estimates "how good" it is to be in a given state.<sup id="cite_ref-:0_14-1" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-:0-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup><sup class="reference nowrap"><span title="Page / location: 60">: 60 </span></sup></p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V_{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid S_{0}=s]=\\operatorname {\\mathbb {E} } \\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}\\mid S_{0}=s\\right],}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow></mrow><mo>⁡</mo><mo stretchy="false">[</mo><mi>G</mi><mo>∣</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub><mo>=</mo><mi>s</mi><mo stretchy="false">]</mo><mo>=</mo><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow></mrow><mo>⁡</mo><mrow><mo>[</mo><mrow><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></munderover><msup><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∣</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub><mo>=</mo><mi>s</mi></mrow><mo>]</mo></mrow><mo>,</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f84b6e9913e812561947e07eb7bee813a398e879" alt="{\\displaystyle V_{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid S_{0}=s]=\\operatorname {\\mathbb {E} } \\left[\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}\\mid S_{0}=s\\right],}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where the random variable<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle G}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>G</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b" alt="{\\displaystyle G}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>denotes the<span>&nbsp;</span><strong>discounted return</strong>, and is defined as the sum of future discounted rewards:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle G=\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}=R_{1}+\\gamma R_{2}+\\gamma ^{2}R_{3}+\\dots ,}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>G</mi><mo>=</mo><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="normal">∞</mi></mrow></munderover><msup><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mrow class="MJX-TeXAtom-ORD"><mn>2</mn></mrow></msup><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mn>3</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>,</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47c6a37cb9ed68a9560efd8521569e05d7378cba" alt="{\\displaystyle G=\\sum _{t=0}^{\\infty }\\gamma ^{t}R_{t+1}=R_{1}+\\gamma R_{2}+\\gamma ^{2}R_{3}+\\dots ,}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{t+1}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/78bc3f077c19f7db4e89b5529a5861b06ae782b7" alt="{\\displaystyle R_{t+1}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the reward for transitioning from state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S_{t}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e2391e6e796fbf718be3828080775ac2ac3d3d4" alt="{\\displaystyle S_{t}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle S_{t+1}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7fde0b9d29bcb7e8d3eee3a8f42ead14aa3b8cee" alt="{\\displaystyle S_{t+1}}" aria-hidden="true" loading="lazy"></span>,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle 0\\leq \\gamma <1}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mn>0</mn><mo>≤</mo><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/84796b874665109176d8773a2e6495f00c7cc360" alt="{\\displaystyle 0\\leq \\gamma <1}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is the<span>&nbsp;</span><a title="Q-learning" href="https://en.wikipedia.org/wiki/Q-learning#Discount_factor">discount rate</a>.<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\gamma }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>γ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a223c880b0ce3da8f64ee33c4f0010beee400b1a" alt="{\\displaystyle \\gamma }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.</p>\n<p>The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called<span>&nbsp;</span><i>stationary</i><span>&nbsp;</span>policies. A policy is<span>&nbsp;</span><i>stationary</i><span>&nbsp;</span>if the action-distribution returned by it depends only on the last state visited (from the observation agent\'s history). The search can be further restricted to<span>&nbsp;</span><i>deterministic</i><span>&nbsp;</span>stationary policies. A<span>&nbsp;</span><i>deterministic stationary</i><span>&nbsp;</span>policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Brute_force">Brute force</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Brute force" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=7"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The<span>&nbsp;</span><a title="Brute-force search" href="https://en.wikipedia.org/wiki/Brute-force_search">brute force</a><span>&nbsp;</span>approach entails two steps:</p>\n<ul>\n<li>For each possible policy, sample returns while following it</li>\n<li>Choose the policy with the largest expected discounted return</li>\n</ul>\n<p>One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.</p>\n<p>These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are<span>&nbsp;</span><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#Value_function">value function estimation</a><span>&nbsp;</span>and<span>&nbsp;</span><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#Direct_policy_search">direct policy search</a>.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Value_function">Value function</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Value function" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=8"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="hatnote navigation-not-searchable" role="note">See also:<span>&nbsp;</span><a title="Value function" href="https://en.wikipedia.org/wiki/Value_function">Value function</a></div>\n<p>Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\operatorname {\\mathbb {E} } [G]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow></mrow><mo>⁡</mo><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false">]</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c17deb2341b047040bbddcbb56bae20d587311cb" alt="{\\displaystyle \\operatorname {\\mathbb {E} } [G]}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>for some policy (usually either the "current" [on-policy] or the optimal [off-policy] one).</p>\n<p>These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from<span>&nbsp;</span><i>any</i><span>&nbsp;</span>initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.</p>\n<p>To define optimality in a formal manner, define the state-value of a policy<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>by</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid s,\\pi ],}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow></mrow><mo>⁡</mo><mo stretchy="false">[</mo><mi>G</mi><mo>∣</mo><mi>s</mi><mo>,</mo><mi>π</mi><mo stretchy="false">]</mo><mo>,</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f79fad295cbb4d688fe27b8e58110fddde33549f" alt="{\\displaystyle V^{\\pi }(s)=\\operatorname {\\mathbb {E} } [G\\mid s,\\pi ],}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle G}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>G</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b" alt="{\\displaystyle G}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>stands for the discounted return associated with following<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>from the initial state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>. Defining<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{*}(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0c55d312621b3ac9cbd28f1af88d01d4cb4d110" alt="{\\displaystyle V^{*}(s)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>as the maximum possible state-value of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{\\pi }(s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a42ff0c4c2e0d6e3bbfc997e81544641805798a4" alt="{\\displaystyle V^{\\pi }(s)}" aria-hidden="true" loading="lazy"></span>, where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is allowed to change,</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{*}(s)=\\max _{\\pi }V^{\\pi }(s).}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo form="prefix" movablelimits="true">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></munder><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>.</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a1364980b05ea95dedcefb2082869eab54dd1cc" alt="{\\displaystyle V^{*}(s)=\\max _{\\pi }V^{\\pi }(s).}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>A policy that achieves these optimal state-values in each state is called<span>&nbsp;</span><i>optimal</i>. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>V</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo form="prefix" movablelimits="true">max</mo><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></munder><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow><mo stretchy="false">[</mo><mi>G</mi><mo>∣</mo><mi>s</mi><mo>,</mo><mi>π</mi><mo stretchy="false">]</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/83afb99d87052615899fbc3413a1e02467143957" alt="{\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}" aria-hidden="true" loading="lazy"></span>, where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is a state randomly sampled from the distribution<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\mu }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>μ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161" alt="{\\displaystyle \\mu }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>of initial states (so<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>μ</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mo form="prefix" movablelimits="true">Pr</mo><mo stretchy="false">(</mo><msub><mi>S</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub><mo>=</mo><mi>s</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/de42c6766fe6d309149deb462214e1d2c9f5fa36" alt="{\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}" aria-hidden="true" loading="lazy"></span>).</p>\n<p>Although state-values suffice to define optimality, it is useful to define action-values. Given a state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>, an action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and a policy<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span>, the action-value of the pair<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e483c36a133514a0cd6d604f5cce56d2fd4cae9" alt="{\\displaystyle (s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>under<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is defined by</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q^{\\pi }(s,a)=\\operatorname {\\mathbb {E} } [G\\mid s,a,\\pi ],\\,}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mi>π</mi></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="double-struck">E</mi></mrow></mrow><mo>⁡</mo><mo stretchy="false">[</mo><mi>G</mi><mo>∣</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>π</mi><mo stretchy="false">]</mo><mo>,</mo><mspace></mspace></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8d16c880426ef6a8e8723bc804fbec99bb81b08" alt="{\\displaystyle Q^{\\pi }(s,a)=\\operatorname {\\mathbb {E} } [G\\mid s,a,\\pi ],\\,}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>where<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle G}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>G</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b" alt="{\\displaystyle G}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>now stands for the random discounted return associated with first taking action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>in state<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>and following<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>π</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9be4ba0bb8df3af72e90a0535fabcc17431e540a" alt="{\\displaystyle \\pi }" aria-hidden="true" loading="lazy"></span>, thereafter.</p>\n<p>The theory of Markov decision processes states that if<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi ^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f44ad69ec033a9a86437b2edaf620ea0b2c3f494" alt="{\\displaystyle \\pi ^{*}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>is an optimal policy, we act optimally (take the optimal action) by choosing the action from<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mrow></msup><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mo>⋅</mo><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fafa0cb84fd68dbe99e9b5baa0b421837c2835d8" alt="{\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>with the highest action-value at each state,<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span>. The<span>&nbsp;</span><i>action-value function</i><span>&nbsp;</span>of such an optimal policy (<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q^{\\pi ^{*}}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><msup><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efdae1153bf5fc57c8777369756d7c5cc20fd90a" alt="{\\displaystyle Q^{\\pi ^{*}}}" aria-hidden="true" loading="lazy"></span>) is called the<span>&nbsp;</span><i>optimal action-value function</i><span>&nbsp;</span>and is commonly denoted by<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c1785c0a77ab5a06684e8a7ac4f5e59d59ec0319" alt="{\\displaystyle Q^{*}}" aria-hidden="true" loading="lazy"></span>. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.</p>\n<p>Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are<span>&nbsp;</span><a class="mw-redirect" title="Value iteration" href="https://en.wikipedia.org/wiki/Value_iteration">value iteration</a><span>&nbsp;</span>and<span>&nbsp;</span><a class="mw-redirect" title="Policy iteration" href="https://en.wikipedia.org/wiki/Policy_iteration">policy iteration</a>. Both algorithms compute a sequence of functions<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q_{k}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mi>k</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2fa6cb6cf7ffc157202e52dd5711e755892d1015" alt="{\\displaystyle Q_{k}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>(<span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle k=0,1,2,\\ldots }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>k</mi><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b6d4b81efe1ce928c56c7b143cb2591f2100246c" alt="{\\displaystyle k=0,1,2,\\ldots }" aria-hidden="true" loading="lazy"></span>) that converge to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q^{*}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>Q</mi><mrow class="MJX-TeXAtom-ORD"><mo>∗</mo></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c1785c0a77ab5a06684e8a7ac4f5e59d59ec0319" alt="{\\displaystyle Q^{*}}" aria-hidden="true" loading="lazy"></span>. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="Monte_Carlo_methods">Monte Carlo methods</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Monte Carlo methods" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=9"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p><a class="mw-redirect" title="Monte Carlo sampling" href="https://en.wikipedia.org/wiki/Monte_Carlo_sampling">Monte Carlo methods</a><sup id="cite_ref-15" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-15"><span class="cite-bracket">[</span>15<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment\'s dynamics, Monte Carlo methods rely solely on actual or<span>&nbsp;</span><a title="Simulation" href="https://en.wikipedia.org/wiki/Simulation">simulated</a><span>&nbsp;</span>experience—sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of<span>&nbsp;</span><a title="Markov chain" href="https://en.wikipedia.org/wiki/Markov_chain">transition probabilities</a>, which is necessary for<span>&nbsp;</span><a title="Dynamic programming" href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a><span>&nbsp;</span>methods.</p>\n<p>Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term "Monte Carlo" generally refers to any method involving<span>&nbsp;</span><a class="mw-redirect" title="Random sampling" href="https://en.wikipedia.org/wiki/Random_sampling">random sampling</a>; however, in this context, it specifically refers to methods that compute averages from<span>&nbsp;</span><i>complete</i><span>&nbsp;</span>returns, rather than<span>&nbsp;</span><i>partial</i><span>&nbsp;</span>returns.</p>\n<p>These methods function similarly to the<span>&nbsp;</span><a title="Multi-armed bandit" href="https://en.wikipedia.org/wiki/Multi-armed_bandit">bandit algorithms</a>, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem<span>&nbsp;</span><a class="mw-redirect" title="Non-stationary" href="https://en.wikipedia.org/wiki/Non-stationary">non-stationary</a>. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes<span>&nbsp;</span><a title="Value function" href="https://en.wikipedia.org/wiki/Value_function">value functions</a><span>&nbsp;</span>using full knowledge of the<span>&nbsp;</span><a title="Markov decision process" href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a><span>&nbsp;</span>(MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve<span>&nbsp;</span><a title="Mathematical optimization" href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimality</a>, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.<sup id="cite_ref-:0_14-2" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-:0-14"><span class="cite-bracket">[</span>14<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading4">\n<h4 id="Temporal_difference_methods">Temporal difference methods</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Temporal difference methods" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=10"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<div class="hatnote navigation-not-searchable" role="note">Main article:<span>&nbsp;</span><a title="Temporal difference learning" href="https://en.wikipedia.org/wiki/Temporal_difference_learning">Temporal difference learning</a></div>\n<p>The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of<span>&nbsp;</span><i>generalized policy iteration</i><span>&nbsp;</span>algorithms. Many<span>&nbsp;</span><a title="Actor-critic algorithm" href="https://en.wikipedia.org/wiki/Actor-critic_algorithm"><i>actor-critic</i><span>&nbsp;</span>methods</a><span>&nbsp;</span>belong to this category.</p>\n<p>The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton\'s<span>&nbsp;</span><a class="mw-redirect" title="Temporal difference" href="https://en.wikipedia.org/wiki/Temporal_difference">temporal difference</a><span>&nbsp;</span>(TD) methods that are based on the recursive<span>&nbsp;</span><a title="Bellman equation" href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>.<sup id="cite_ref-16" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-16"><span class="cite-bracket">[</span>16<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_§6._Temporal-Difference_Learning]_17-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-FOOTNOTESuttonBarto2018[httpincompleteideasnetsuttonbookebooknode60html_%C2%A76._Temporal-Difference_Learning]-17"><span class="cite-bracket">[</span>17<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,<sup id="cite_ref-18" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-18"><span class="cite-bracket">[</span>18<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.</p>\n<p>Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\lambda }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>λ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a" alt="{\\displaystyle \\lambda }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>parameter<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (0\\leq \\lambda \\leq 1)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mn>0</mn><mo>≤</mo><mi>λ</mi><mo>≤</mo><mn>1</mn><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/65907c1d6a086eeb039ebeca34502f7463d06b56" alt="{\\displaystyle (0\\leq \\lambda \\leq 1)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.</p>\n<div class="mw-heading mw-heading4">\n<h4 id="Function_approximation_methods">Function approximation methods</h4>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Function approximation methods" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=11"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In order to address the fifth issue,<span>&nbsp;</span><i>function approximation methods</i><span>&nbsp;</span>are used.<span>&nbsp;</span><i>Linear function approximation</i><span>&nbsp;</span>starts with a mapping<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\phi }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ϕ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72b1f30316670aee6270a28334bdf4f5072cdde4" alt="{\\displaystyle \\phi }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle (s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e483c36a133514a0cd6d604f5cce56d2fd4cae9" alt="{\\displaystyle (s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>are obtained by linearly combining the components of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\phi (s,a)}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ϕ</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/679d681f66578f81accbbebe2598038097c95965" alt="{\\displaystyle \\phi (s,a)}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>with some<span>&nbsp;</span><i>weights</i><span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\theta }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>θ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="{\\displaystyle \\theta }" aria-hidden="true" loading="lazy"></span>:</p>\n<dl>\n<dd><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle Q(s,a)=\\sum _{i=1}^{d}\\theta _{i}\\phi _{i}(s,a).}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi></mrow></munderover><msub><mi>θ</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><msub><mi>ϕ</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>.</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8fb9c17e9850dfa123aac5cf0541b629df47de7" alt="{\\displaystyle Q(s,a)=\\sum _{i=1}^{d}\\theta _{i}\\phi _{i}(s,a).}" aria-hidden="true" loading="lazy"></span></dd>\n</dl>\n<p>The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from<span>&nbsp;</span><a title="Nonparametric statistics" href="https://en.wikipedia.org/wiki/Nonparametric_statistics">nonparametric statistics</a><span>&nbsp;</span>(which can be seen to construct their own features) have been explored.</p>\n<p>Value iteration can also be used as a starting point, giving rise to the<span>&nbsp;</span><a title="Q-learning" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a><span>&nbsp;</span>algorithm and its many variants.<sup id="cite_ref-19" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-19"><span class="cite-bracket">[</span>19<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.<sup id="cite_ref-MBK_20-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-MBK-20"><span class="cite-bracket">[</span>20<span class="cite-bracket">]</span></a></sup></p>\n<p>The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Direct_policy_search">Direct policy search</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Direct policy search" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=12"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of<span>&nbsp;</span><a title="Stochastic optimization" href="https://en.wikipedia.org/wiki/Stochastic_optimization">stochastic optimization</a>. The two approaches available are gradient-based and gradient-free methods.</p>\n<p><a title="Gradient" href="https://en.wikipedia.org/wiki/Gradient">Gradient</a>-based methods (<i>policy gradient methods</i>) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\theta }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>θ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="{\\displaystyle \\theta }" aria-hidden="true" loading="lazy"></span>, let<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\pi _{\\theta }}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mi>θ</mi></mrow></msub></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a9f6266b9302e47ccd37e867f4a364ff52fb8af" alt="{\\displaystyle \\pi _{\\theta }}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>denote the policy associated to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\theta }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>θ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="{\\displaystyle \\theta }" aria-hidden="true" loading="lazy"></span>. Defining the performance function by<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ρ</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>ρ</mi><mrow class="MJX-TeXAtom-ORD"><msub><mi>π</mi><mrow class="MJX-TeXAtom-ORD"><mi>θ</mi></mrow></msub></mrow></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a66bf6220d37e57309930a00168cf36d105393b9" alt="{\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>under mild conditions this function will be differentiable as a function of the parameter vector<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\theta }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>θ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" alt="{\\displaystyle \\theta }" aria-hidden="true" loading="lazy"></span>. If the gradient of<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle \\rho }"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>ρ</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" alt="{\\displaystyle \\rho }" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>was known, one could use<span>&nbsp;</span><a title="Gradient descent" href="https://en.wikipedia.org/wiki/Gradient_descent">gradient ascent</a>. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams\'s REINFORCE method<sup id="cite_ref-21" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-21"><span class="cite-bracket">[</span>21<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>(which is known as the likelihood ratio method in the<span>&nbsp;</span><a title="Simulation-based optimization" href="https://en.wikipedia.org/wiki/Simulation-based_optimization">simulation-based optimization</a><span>&nbsp;</span>literature).<sup id="cite_ref-22" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-22"><span class="cite-bracket">[</span>22<span class="cite-bracket">]</span></a></sup></p>\n<p>A large class of methods avoids relying on gradient information. These include<span>&nbsp;</span><a title="Simulated annealing" href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a>,<span>&nbsp;</span><a title="Cross-entropy method" href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy search</a><span>&nbsp;</span>or methods of<span>&nbsp;</span><a title="Evolutionary computation" href="https://en.wikipedia.org/wiki/Evolutionary_computation">evolutionary computation</a>. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.</p>\n<p>Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years,<span>&nbsp;</span><i>actor–critic methods</i><span>&nbsp;</span>have been proposed and performed well on various problems.<sup id="cite_ref-23" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-23"><span class="cite-bracket">[</span>23<span class="cite-bracket">]</span></a></sup></p>\n<p>Policy search methods have been used in the<span>&nbsp;</span><a title="Robotics" href="https://en.wikipedia.org/wiki/Robotics">robotics</a><span>&nbsp;</span>context.<sup id="cite_ref-24" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-24"><span class="cite-bracket">[</span>24<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Many policy search methods may get stuck in local optima (as they are based on<span>&nbsp;</span><a title="Local search (optimization)" href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local search</a>).</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Model-based_algorithms">Model-based algorithms</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Model-based algorithms" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=13"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Finally, all of the above methods can be combined with algorithms that first learn a model of the<span>&nbsp;</span><a title="Markov decision process" href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a>, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.<sup id="cite_ref-25" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-25"><span class="cite-bracket">[</span>25<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and "replayed" to the learning algorithm.<sup id="cite_ref-26" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-26"><span class="cite-bracket">[</span>26<span class="cite-bracket">]</span></a></sup></p>\n<p>Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov decision process can be learnt.<sup id="cite_ref-27" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-27"><span class="cite-bracket">[</span>27<span class="cite-bracket">]</span></a></sup></p>\n<p>There are other ways to use models than to update a value function.<sup id="cite_ref-28" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-28"><span class="cite-bracket">[</span>28<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>For instance, in<span>&nbsp;</span><a title="Model predictive control" href="https://en.wikipedia.org/wiki/Model_predictive_control">model predictive control</a><span>&nbsp;</span>the model is used to update the behavior directly.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Theory">Theory</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Theory" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=14"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.</p>\n<p>Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997).<sup id="cite_ref-Optimal_adaptive_policies_for_Marko_12-1" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-Optimal_adaptive_policies_for_Marko-12"><span class="cite-bracket">[</span>12<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.</p>\n<p>For incremental algorithms, asymptotic convergence issues have been settled.<sup class="noprint Inline-Template">[<i><a title="Wikipedia:Please clarify" href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify"><span title="What are the issues that have been settled? (January 2020)">clarification needed</span></a></i>]</sup><span>&nbsp;</span>Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Research">Research</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Research" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=15"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<table class="box-More_citations_needed_section plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation">\n<tbody>\n<tr>\n<td class="mbox-image">\n<div class="mbox-image-div"><span><a class="mw-file-description" href="https://en.wikipedia.org/wiki/File:Question_book-new.svg"><img class="mw-file-element" src="https://upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/60px-Question_book-new.svg.png" alt="" width="50" height="39" data-file-width="512" data-file-height="399" loading="lazy"></a></span></div>\n</td>\n<td class="mbox-text">\n<div class="mbox-text-span">This section<span>&nbsp;</span><strong>needs additional citations for<span>&nbsp;</span><a title="Wikipedia:Verifiability" href="https://en.wikipedia.org/wiki/Wikipedia:Verifiability">verification</a></strong>.<span class="hide-when-compact"><span>&nbsp;</span>Please help<span>&nbsp;</span><a title="Special:EditPage/Reinforcement learning" href="https://en.wikipedia.org/wiki/Special:EditPage/Reinforcement_learning">improve this article</a><span>&nbsp;</span>by<span>&nbsp;</span><a title="Help:Referencing for beginners" href="https://en.wikipedia.org/wiki/Help:Referencing_for_beginners">adding citations to reliable sources</a><span>&nbsp;</span>in this section. Unsourced material may be challenged and removed.</span><span>&nbsp;</span><span class="date-container"><i>(<span class="date">October 2022</span>)</i></span><span class="hide-when-compact"><i><span>&nbsp;</span>(<small><a title="Help:Maintenance template removal" href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal">Learn how and when to remove this message</a></small>)</i></span></div>\n</td>\n</tr>\n</tbody>\n</table>\n<p>Research topics include:</p>\n<ul>\n<li>actor-critic architecture<sup id="cite_ref-29" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-29"><span class="cite-bracket">[</span>29<span class="cite-bracket">]</span></a></sup></li>\n<li>actor-critic-scenery architecture<sup id="cite_ref-Li-2023_3-1" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-Li-2023-3"><span class="cite-bracket">[</span>3<span class="cite-bracket">]</span></a></sup></li>\n<li>adaptive methods that work with fewer (or no) parameters under a large number of conditions</li>\n<li>bug detection in software projects<sup id="cite_ref-30" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-30"><span class="cite-bracket">[</span>30<span class="cite-bracket">]</span></a></sup></li>\n<li>continuous learning</li>\n<li>combinations with logic-based frameworks<sup id="cite_ref-31" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-31"><span class="cite-bracket">[</span>31<span class="cite-bracket">]</span></a></sup></li>\n<li>exploration in large Markov decision processes</li>\n<li>entity-based reinforcement learning<sup id="cite_ref-32" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-32"><span class="cite-bracket">[</span>32<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-33" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-33"><span class="cite-bracket">[</span>33<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-34" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-34"><span class="cite-bracket">[</span>34<span class="cite-bracket">]</span></a></sup></li>\n<li><a title="Reinforcement learning from human feedback" href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">human feedback</a><sup id="cite_ref-35" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-35"><span class="cite-bracket">[</span>35<span class="cite-bracket">]</span></a></sup></li>\n<li>interaction between implicit and explicit learning in skill acquisition</li>\n<li><a title="Intrinsic motivation (artificial intelligence)" href="https://en.wikipedia.org/wiki/Intrinsic_motivation_(artificial_intelligence)">intrinsic motivation</a><span>&nbsp;</span>which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations</li>\n<li>large (or continuous) action spaces</li>\n<li>modular and hierarchical reinforcement learning<sup id="cite_ref-36" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-36"><span class="cite-bracket">[</span>36<span class="cite-bracket">]</span></a></sup></li>\n<li>multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.<sup id="cite_ref-37" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-37"><span class="cite-bracket">[</span>37<span class="cite-bracket">]</span></a></sup></li>\n<li>occupant-centric control</li>\n<li>optimization of computing resources<sup id="cite_ref-38" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-38"><span class="cite-bracket">[</span>38<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-39" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-39"><span class="cite-bracket">[</span>39<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-40" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-40"><span class="cite-bracket">[</span>40<span class="cite-bracket">]</span></a></sup></li>\n<li><a title="Partially observable Markov decision process" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">partial information</a><span>&nbsp;</span>(e.g., using<span>&nbsp;</span><a title="Predictive state representation" href="https://en.wikipedia.org/wiki/Predictive_state_representation">predictive state representation</a>)</li>\n<li>reward function based on maximising novel information<sup id="cite_ref-kaplan2004_41-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-kaplan2004-41"><span class="cite-bracket">[</span>41<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-klyubin2008_42-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-klyubin2008-42"><span class="cite-bracket">[</span>42<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-barto2013_43-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-barto2013-43"><span class="cite-bracket">[</span>43<span class="cite-bracket">]</span></a></sup></li>\n<li>sample-based planning (e.g., based on<span>&nbsp;</span><a title="Monte Carlo tree search" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a>).</li>\n<li>securities trading<sup id="cite_ref-44" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-44"><span class="cite-bracket">[</span>44<span class="cite-bracket">]</span></a></sup></li>\n<li><a title="Transfer learning" href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a><sup id="cite_ref-45" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-45"><span class="cite-bracket">[</span>45<span class="cite-bracket">]</span></a></sup></li>\n<li>TD learning modeling<span>&nbsp;</span><a title="Dopamine" href="https://en.wikipedia.org/wiki/Dopamine">dopamine</a>-based learning in the brain.<span>&nbsp;</span><a title="Dopaminergic" href="https://en.wikipedia.org/wiki/Dopaminergic">Dopaminergic</a><span>&nbsp;</span>projections from the<span>&nbsp;</span><a title="Substantia nigra" href="https://en.wikipedia.org/wiki/Substantia_nigra">substantia nigra</a><span>&nbsp;</span>to the<span>&nbsp;</span><a title="Basal ganglia" href="https://en.wikipedia.org/wiki/Basal_ganglia">basal ganglia</a><span>&nbsp;</span>function are the prediction error.</li>\n<li>value-function and policy search methods</li>\n</ul>\n<div class="mw-heading mw-heading2">\n<h2 id="Comparison_of_key_algorithms">Comparison of key algorithms</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Comparison of key algorithms" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=16"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The following table lists the key algorithms for learning a policy depending on several criteria:</p>\n<ul>\n<li>The algorithm can be on-policy (it performs policy updates using trajectories sampled via the current policy)<sup id="cite_ref-46" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-46"><span class="cite-bracket">[</span>46<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>or off-policy.</li>\n<li>The action space may be discrete (e.g. the action space could be "going up", "going left", "going right", "going down", "stay") or continuous (e.g. moving the arm with a given angle).</li>\n<li>The state space may be discrete (e.g. the agent could be in a cell in a grid) or continuous (e.g. the agent could be located at a given position in the plane).</li>\n</ul>\n<table class="wikitable sortable jquery-tablesorter">\n<thead>\n<tr>\n<th class="headerSort" title="Sort ascending" role="columnheader button">Algorithm</th>\n<th class="headerSort" title="Sort ascending" role="columnheader button">Description</th>\n<th class="headerSort" title="Sort ascending" role="columnheader button">Policy</th>\n<th class="headerSort" title="Sort ascending" role="columnheader button">Action space</th>\n<th class="headerSort" title="Sort ascending" role="columnheader button">State space</th>\n<th class="headerSort" title="Sort ascending" role="columnheader button">Operator</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a title="Monte Carlo method" href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a></td>\n<td>Every visit to Monte Carlo</td>\n<td>Either</td>\n<td>Discrete</td>\n<td>Discrete</td>\n<td>Sample-means of state-values or action-values</td>\n</tr>\n<tr>\n<td><a title="Temporal difference learning" href="https://en.wikipedia.org/wiki/Temporal_difference_learning">TD learning</a></td>\n<td>State–action–reward–state</td>\n<td>Off-policy</td>\n<td>Discrete</td>\n<td>Discrete</td>\n<td>State-value</td>\n</tr>\n<tr>\n<td><a title="Q-learning" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a></td>\n<td>State–action–reward–state</td>\n<td>Off-policy</td>\n<td>Discrete</td>\n<td>Discrete</td>\n<td>Action-value</td>\n</tr>\n<tr>\n<td><a title="State–action–reward–state–action" href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action">SARSA</a></td>\n<td>State–action–reward–state–action</td>\n<td>On-policy</td>\n<td>Discrete</td>\n<td>Discrete</td>\n<td>Action-value</td>\n</tr>\n<tr>\n<td><a title="Q-learning" href="https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning">DQN</a></td>\n<td>Deep Q Network</td>\n<td>Off-policy</td>\n<td>Discrete</td>\n<td>Continuous</td>\n<td>Action-value</td>\n</tr>\n<tr>\n<td>DDPG</td>\n<td>Deep Deterministic Policy Gradient</td>\n<td>Off-policy</td>\n<td>Continuous</td>\n<td>Continuous</td>\n<td>Action-value</td>\n</tr>\n<tr>\n<td>A3C</td>\n<td>Asynchronous Advantage Actor-Critic Algorithm</td>\n<td>On-policy</td>\n<td>Discrete</td>\n<td>Continuous</td>\n<td>Advantage (=action-value - state-value)</td>\n</tr>\n<tr>\n<td>TRPO</td>\n<td>Trust Region Policy Optimization</td>\n<td>On-policy</td>\n<td>Continuous or Discrete</td>\n<td>Continuous</td>\n<td>Advantage</td>\n</tr>\n<tr>\n<td><a class="mw-redirect" title="Proximal Policy Optimization" href="https://en.wikipedia.org/wiki/Proximal_Policy_Optimization">PPO</a></td>\n<td>Proximal Policy Optimization</td>\n<td>On-policy</td>\n<td>Continuous or Discrete</td>\n<td>Continuous</td>\n<td>Advantage</td>\n</tr>\n<tr>\n<td>TD3</td>\n<td>Twin Delayed Deep Deterministic Policy Gradient</td>\n<td>Off-policy</td>\n<td>Continuous</td>\n<td>Continuous</td>\n<td>Action-value</td>\n</tr>\n<tr>\n<td>SAC</td>\n<td>Soft Actor-Critic</td>\n<td>Off-policy</td>\n<td>Continuous</td>\n<td>Continuous</td>\n<td>Advantage</td>\n</tr>\n<tr>\n<td><a title="Distributional Soft Actor Critic" href="https://en.wikipedia.org/wiki/Distributional_Soft_Actor_Critic">DSAC</a><sup id="cite_ref-47" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-47"><span class="cite-bracket">[</span>47<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-48" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-48"><span class="cite-bracket">[</span>48<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-49" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-49"><span class="cite-bracket">[</span>49<span class="cite-bracket">]</span></a></sup></td>\n<td>Distributional Soft Actor Critic</td>\n<td>Off-policy</td>\n<td>Continuous</td>\n<td>Continuous</td>\n<td>Action-value distribution</td>\n</tr>\n</tbody>\n<tfoot></tfoot>\n</table>\n<div class="mw-heading mw-heading3">\n<h3 id="Associative_reinforcement_learning">Associative reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Associative reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=17"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.<sup id="cite_ref-50" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-50"><span class="cite-bracket">[</span>50<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Deep_reinforcement_learning">Deep reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Deep reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=18"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.<sup id="cite_ref-intro_deep_RL_51-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-intro_deep_RL-51"><span class="cite-bracket">[</span>51<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>The work on learning ATARI games by Google<span>&nbsp;</span><a class="mw-redirect" title="DeepMind" href="https://en.wikipedia.org/wiki/DeepMind">DeepMind</a><span>&nbsp;</span>increased attention to<span>&nbsp;</span><a title="Deep reinforcement learning" href="https://en.wikipedia.org/wiki/Deep_reinforcement_learning">deep reinforcement learning</a><span>&nbsp;</span>or<span>&nbsp;</span><a class="mw-redirect" title="End-to-end reinforcement learning" href="https://en.wikipedia.org/wiki/End-to-end_reinforcement_learning">end-to-end reinforcement learning</a>.<sup id="cite_ref-DQN2_52-0" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-DQN2-52"><span class="cite-bracket">[</span>52<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Adversarial_deep_reinforcement_learning">Adversarial deep reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Adversarial deep reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=19"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.<sup id="cite_ref-53" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-53"><span class="cite-bracket">[</span>53<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-54" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-54"><span class="cite-bracket">[</span>54<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-55" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-55"><span class="cite-bracket">[</span>55<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.<sup id="cite_ref-56" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-56"><span class="cite-bracket">[</span>56<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Fuzzy_reinforcement_learning">Fuzzy reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Fuzzy reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=20"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>By introducing<span>&nbsp;</span><a title="Fuzzy control system" href="https://en.wikipedia.org/wiki/Fuzzy_control_system">fuzzy inference</a><span>&nbsp;</span>in reinforcement learning,<sup id="cite_ref-57" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-57"><span class="cite-bracket">[</span>57<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>approximating the state-action value function with<span>&nbsp;</span><a title="Fuzzy rule" href="https://en.wikipedia.org/wiki/Fuzzy_rule">fuzzy rules</a><span>&nbsp;</span>in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation<sup id="cite_ref-58" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-58"><span class="cite-bracket">[</span>58<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Inverse_reinforcement_learning">Inverse reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Inverse reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=21"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.<sup id="cite_ref-59" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-59"><span class="cite-bracket">[</span>59<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL).<sup id="cite_ref-60" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-60"><span class="cite-bracket">[</span>60<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL).<sup id="cite_ref-61" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-61"><span class="cite-bracket">[</span>61<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>RU-IRL is based on<span>&nbsp;</span><a title="Random utility model" href="https://en.wikipedia.org/wiki/Random_utility_model">random utility theory</a><span>&nbsp;</span>and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Multi-objective_reinforcement_learning">Multi-objective reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Multi-objective reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=22"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Multi-objective reinforcement learning (MORL) is a form of reinforcement learning concerned with conflicting alternatives. It is distinct from multi-objective optimization in that it is concerned with agents acting in environments.<sup id="cite_ref-62" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-62"><span class="cite-bracket">[</span>62<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-63" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-63"><span class="cite-bracket">[</span>63<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Safe_reinforcement_learning">Safe reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Safe reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=23"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.<sup id="cite_ref-64" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-64"><span class="cite-bracket">[</span>64<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>An alternative approach is risk-averse reinforcement learning, where instead of the<span>&nbsp;</span><i>expected</i><span>&nbsp;</span>return, a<span>&nbsp;</span><i>risk-measure</i><span>&nbsp;</span>of the return is optimized, such as the<span>&nbsp;</span><a title="Expected shortfall" href="https://en.wikipedia.org/wiki/Expected_shortfall">conditional value at risk</a><span>&nbsp;</span>(CVaR).<sup id="cite_ref-65" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-65"><span class="cite-bracket">[</span>65<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.<sup id="cite_ref-66" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-66"><span class="cite-bracket">[</span>66<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-67" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-67"><span class="cite-bracket">[</span>67<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias<sup id="cite_ref-68" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-68"><span class="cite-bracket">[</span>68<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>and blindness to success.<sup id="cite_ref-69" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-69"><span class="cite-bracket">[</span>69<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Self-reinforcement_learning">Self-reinforcement learning</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Self-reinforcement learning" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=24"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Self-reinforcement learning (or self-learning), is a learning paradigm which does not use the concept of immediate reward<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle R_{a}(s,s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msub><mi>R</mi><mrow class="MJX-TeXAtom-ORD"><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f842d4a4b1340e194d8014d63163e5e27f94215" alt="{\\displaystyle R_{a}(s,s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>after transition from<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>to<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>with action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span>. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation.</p>\n<p>The self-reinforcement algorithm updates a memory matrix<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle W=||w(a,s)||}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>W</mi><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/83684b4f58ec6b7a48f7341a50853c0ba10fe335" alt="{\\displaystyle W=||w(a,s)||}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>such that in each iteration executes the following machine learning routine:</p>\n<ol>\n<li>In situation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>s</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" alt="{\\displaystyle s}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>perform action<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle a}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>a</mi></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc" alt="{\\displaystyle a}" aria-hidden="true" loading="lazy"></span>.</li>\n<li>Receive a consequence situation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span>.</li>\n<li>Compute state evaluation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle v(s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><mi>v</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ecf627d0e9748f5a2a33f97488e76dbb2c939324" alt="{\\displaystyle v(s\')}" aria-hidden="true" loading="lazy"></span><span>&nbsp;</span>of how good is to be in the consequence situation<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle s\'}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>s</mi><mo>′</mo></msup></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5136680c63706cfd17ceddb4acddbfdd0ba5ef2d" alt="{\\displaystyle s\'}" aria-hidden="true" loading="lazy"></span>.</li>\n<li>Update crossbar memory<span>&nbsp;</span><span class="mwe-math-element mwe-math-element-inline"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\\displaystyle w\'(a,s)=w(a,s)+v(s\')}"><semantics><mrow class="MJX-TeXAtom-ORD"><mstyle scriptlevel="0" displaystyle="true"><msup><mi>w</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mi>w</mi><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>s</mi><mo stretchy="false">)</mo><mo>+</mo><mi>v</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mstyle></mrow></semantics></math></span><img class="mwe-math-fallback-image-inline mw-invert skin-invert" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/20975b3e3137f878d335f418c87381fd44cf1e4d" alt="{\\displaystyle w\'(a,s)=w(a,s)+v(s\')}" aria-hidden="true" loading="lazy"></span>.</li>\n</ol>\n<p>Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior).</p>\n<p>Self-reinforcement (self-learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA).<sup id="cite_ref-70" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-70"><span class="cite-bracket">[</span>70<span class="cite-bracket">]</span></a></sup><sup id="cite_ref-71" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-71"><span class="cite-bracket">[</span>71<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion.<sup id="cite_ref-72" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-72"><span class="cite-bracket">[</span>72<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading3">\n<h3 id="Reinforcement_Learning_in_Natural_Language_Processing">Reinforcement Learning in Natural Language Processing</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Reinforcement Learning in Natural Language Processing" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=25"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>In recent years, Reinforcement learning has become a significant concept in<span>&nbsp;</span><a title="Natural language processing" href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing (NLP)</a>, where tasks are often sequential decision-making rather than static classification. Reinforcement learning is where an agent take actions in an environment to maximize the accumulation of rewards. This framework is best fit for many NLP tasks, including dialogue generation, text summarization, and machine translation, where the quality of the output depends on optimizing long-term or human-centered goals rather than the prediction of single correct label.</p>\n<p>Early application of RL in NLP emerged in dialogue systems, where conversation was determined as a series of actions optimized for fluency and coherence. These early attempts, including policy gradient and sequence-level training techniques, laid a foundation for the broader application of reinforcement learning to other areas of NLP.</p>\n<p>A major breakthrough happened with the introduction of<span>&nbsp;</span><a title="Reinforcement learning from human feedback" href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement Learning from Human Feedback (RLHF)</a>, a method in which human feedbacks are used to train a reward model that guides the RL agent. Unlike traditional rule-based or supervised systems, RLHF allows models to align their behavior with human judgments on complex and subjective tasks. This technique was initially used in the development of<span>&nbsp;</span><a class="mw-redirect" title="InstructGPT" href="https://en.wikipedia.org/wiki/InstructGPT">InstructGPT</a>, an effective language model trained to follow human instructions and later in<span>&nbsp;</span><a title="ChatGPT" href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a><span>&nbsp;</span>which incorporates RLHF for improving output responses and ensuring safety.</p>\n<p>More recently, researchers have explored the use of offline RL in NLP to improve dialogue systems without the need of live human interaction. These methods optimize for user engagement, coherence, and diversity based on past conversation logs and pre-trained reward models.</p>\n<div class="mw-heading mw-heading2">\n<h2 id="Statistical_comparison_of_reinforcement_learning_algorithms">Statistical comparison of reinforcement learning algorithms</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Statistical comparison of reinforcement learning algorithms" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=26"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.<sup id="cite_ref-73" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-73"><span class="cite-bracket">[</span>73<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be<span>&nbsp;</span><a class="mw-redirect" title="I.i.d" href="https://en.wikipedia.org/wiki/I.i.d">i.i.d</a>, standard statistical tools can be used for hypothesis testing, such as<span>&nbsp;</span><a title="Student\'s t-test" href="https://en.wikipedia.org/wiki/Student%27s_t-test">T-test</a><span>&nbsp;</span>and<span>&nbsp;</span><a title="Permutation test" href="https://en.wikipedia.org/wiki/Permutation_test">permutation test</a>.<sup id="cite_ref-74" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-74"><span class="cite-bracket">[</span>74<span class="cite-bracket">]</span></a></sup><span>&nbsp;</span>This requires to accumulate all the rewards within an episode into a single number—the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.<sup id="cite_ref-75" class="reference"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning#cite_note-75"><span class="cite-bracket">[</span>75<span class="cite-bracket">]</span></a></sup></p>\n<div class="mw-heading mw-heading2">\n<h2 id="Challenges_and_Limitations">Challenges and Limitations</h2>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Challenges and Limitations" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=27"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Despite significant advancements, reinforcement learning (RL) continues to face several challenges and limitations that hinder its widespread application in real-world scenarios.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Sample_Inefficiency">Sample Inefficiency</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Sample Inefficiency" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=28"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>RL algorithms often require a large number of interactions with the environment to learn effective policies, leading to high computational costs and time-intensive to train the agent. For instance,<span>&nbsp;</span><a title="OpenAI" href="https://en.wikipedia.org/wiki/OpenAI">OpenAI\'s</a><span>&nbsp;</span>Dota-playing bot utilized thousands of years of simulated gameplay to achieve human-level performance. Techniques like experience replay and<span>&nbsp;</span><a title="Curriculum learning" href="https://en.wikipedia.org/wiki/Curriculum_learning">curriculum learning</a><span>&nbsp;</span>have been proposed to deprive sample inefficiency, but these techniques add more complexity and are not always sufficient for real-world applications.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Stability_and_Convergence_Issues">Stability and Convergence Issues</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Stability and Convergence Issues" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=29"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Training RL models, particularly for<span>&nbsp;</span><a title="Deep learning" href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network-based models</a>, can be unstable and prone to divergence. A small change in the policy or environment can lead to extreme fluctuations in performance, making it difficult to achieve consistent results. This instability is further enhanced in the case of the continuous or high-dimensional action space, where the learning step becomes more complex and less predictable.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Generalization_and_Transferability">Generalization and Transferability</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Generalization and Transferability" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=30"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>The RL agents trained in specific environments often struggle to generalize their learned policies to new, unseen scenarios. This is the major setback preventing the application of RL to dynamic real-world environments where adaptability is crucial. The challenge is to develop such algorithms that can transfer knowledge across tasks and environments without extensive retraining.</p>\n<div class="mw-heading mw-heading3">\n<h3 id="Bias_and_Reward_Function_Issues">Bias and Reward Function Issues</h3>\n<span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a title="Edit section: Bias and Reward Function Issues" href="https://en.wikipedia.org/w/index.php?title=Reinforcement_learning&amp;action=edit&amp;section=31"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></div>\n<p>Designing appropriate reward functions is critical in RL because poorly designed reward functions can lead to unintended behaviors. In addition, RL systems trained on biased data may perpetuate existing biases and lead to discriminatory or unfair outcomes. Both of these issues requires careful consideration of reward structures and data sources to ensure fairness and desired behaviors.</p>\n</div>\n</div>\n</div>\n</div>\n<div class="mw-footer-container"><footer id="footer" class="mw-footer">\n<ul id="footer-icons" class="noprint">\n<li id="footer-poweredbyico"></li>\n</ul>\n</footer></div>\n</div>\n</div><script src="https://instructure-uploads-eu.s3.eu-west-1.amazonaws.com/account_107380000000000001/attachments/3521935/dp_app.js"></script>'
